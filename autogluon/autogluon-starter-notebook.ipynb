{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f278e8f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:14:43.822926Z",
     "iopub.status.busy": "2024-09-10T17:14:43.822055Z",
     "iopub.status.idle": "2024-09-10T17:14:43.826992Z",
     "shell.execute_reply": "2024-09-10T17:14:43.826237Z"
    },
    "papermill": {
     "duration": 0.014041,
     "end_time": "2024-09-10T17:14:43.828980",
     "exception": false,
     "start_time": "2024-09-10T17:14:43.814939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspired from Gaurav Duttas work in competition :  Binary Prediction of Poisonous Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713cde1",
   "metadata": {
    "papermill": {
     "duration": 0.004597,
     "end_time": "2024-09-10T17:14:43.838818",
     "exception": false,
     "start_time": "2024-09-10T17:14:43.834221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🚗 **2024 Kaggle Playground Series: Used Car Price Prediction** 🏁\n",
    "\n",
    "Welcome to the 2024 Kaggle Playground Series! 🎉 In this competition, we will predict the price of used cars using various attributes. The goal is to create a robust model using **AutoGluon**, leveraging the power of GPUs for efficient computation. Let's dive in!\n",
    "\n",
    "## 📚 **Libraries and Setup**\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use AutoGluon for its ease of use and efficiency in model training and deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e586e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:14:43.849538Z",
     "iopub.status.busy": "2024-09-10T17:14:43.849239Z",
     "iopub.status.idle": "2024-09-10T17:15:44.374488Z",
     "shell.execute_reply": "2024-09-10T17:15:44.373336Z"
    },
    "papermill": {
     "duration": 60.533618,
     "end_time": "2024-09-10T17:15:44.377133",
     "exception": false,
     "start_time": "2024-09-10T17:14:43.843515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ray==2.10.0 autogluon.tabular ipywidgets catboost==1.2.5 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5689438a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-10T17:15:44.389973Z",
     "iopub.status.busy": "2024-09-10T17:15:44.389620Z",
     "iopub.status.idle": "2024-09-10T17:15:47.860014Z",
     "shell.execute_reply": "2024-09-10T17:15:47.859205Z"
    },
    "papermill": {
     "duration": 3.479479,
     "end_time": "2024-09-10T17:15:47.862526",
     "exception": false,
     "start_time": "2024-09-10T17:15:44.383047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c8faf",
   "metadata": {
    "papermill": {
     "duration": 0.005012,
     "end_time": "2024-09-10T17:15:47.872872",
     "exception": false,
     "start_time": "2024-09-10T17:15:47.867860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 💾 **Load and Explore the Data**\n",
    "\n",
    "Let's load the dataset and take a quick look at the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f37ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:15:47.884766Z",
     "iopub.status.busy": "2024-09-10T17:15:47.883886Z",
     "iopub.status.idle": "2024-09-10T17:15:49.433464Z",
     "shell.execute_reply": "2024-09-10T17:15:49.432450Z"
    },
    "papermill": {
     "duration": 1.557963,
     "end_time": "2024-09-10T17:15:49.435737",
     "exception": false,
     "start_time": "2024-09-10T17:15:47.877774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MINI</td>\n",
       "      <td>Cooper S Base</td>\n",
       "      <td>2007</td>\n",
       "      <td>213000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>LS V8</td>\n",
       "      <td>2002</td>\n",
       "      <td>143250</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Beige</td>\n",
       "      <td>At least 1 accident or damage reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Silverado 2500 LT</td>\n",
       "      <td>2002</td>\n",
       "      <td>136731</td>\n",
       "      <td>E85 Flex Fuel</td>\n",
       "      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>G90 5.0 Ultimate</td>\n",
       "      <td>2017</td>\n",
       "      <td>19500</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>Transmission w/Dual Shift Mode</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>45000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>Metris Base</td>\n",
       "      <td>2021</td>\n",
       "      <td>7388</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>7-Speed A/T</td>\n",
       "      <td>Black</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>97500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id          brand              model  model_year  milage      fuel_type  \\\n",
       "0   0           MINI      Cooper S Base        2007  213000       Gasoline   \n",
       "1   1        Lincoln              LS V8        2002  143250       Gasoline   \n",
       "2   2      Chevrolet  Silverado 2500 LT        2002  136731  E85 Flex Fuel   \n",
       "3   3        Genesis   G90 5.0 Ultimate        2017   19500       Gasoline   \n",
       "4   4  Mercedes-Benz        Metris Base        2021    7388       Gasoline   \n",
       "\n",
       "                                              engine  \\\n",
       "0       172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n",
       "1       252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n",
       "2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...   \n",
       "3       420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n",
       "4       208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n",
       "\n",
       "                     transmission ext_col int_col  \\\n",
       "0                             A/T  Yellow    Gray   \n",
       "1                             A/T  Silver   Beige   \n",
       "2                             A/T    Blue    Gray   \n",
       "3  Transmission w/Dual Shift Mode   Black   Black   \n",
       "4                     7-Speed A/T   Black   Beige   \n",
       "\n",
       "                                 accident clean_title  price  \n",
       "0                           None reported         Yes   4200  \n",
       "1  At least 1 accident or damage reported         Yes   4999  \n",
       "2                           None reported         Yes  13900  \n",
       "3                           None reported         Yes  45000  \n",
       "4                           None reported         Yes  97500  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv')\n",
    "test=pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f58b093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:15:49.448217Z",
     "iopub.status.busy": "2024-09-10T17:15:49.447841Z",
     "iopub.status.idle": "2024-09-10T17:15:49.704426Z",
     "shell.execute_reply": "2024-09-10T17:15:49.703473Z"
    },
    "papermill": {
     "duration": 0.26537,
     "end_time": "2024-09-10T17:15:49.706803",
     "exception": false,
     "start_time": "2024-09-10T17:15:49.441433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    188533\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.duplicated().value_counts() # no duplicates :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974dadd",
   "metadata": {
    "papermill": {
     "duration": 0.006006,
     "end_time": "2024-09-10T17:15:49.719547",
     "exception": false,
     "start_time": "2024-09-10T17:15:49.713541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🚀 **Training the Model with AutoGluon**\n",
    "\n",
    "Now, we'll train the model using AutoGluon, which automatically handles feature engineering, model selection, and hyperparameter tuning. We'll utilize the GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd32f598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:15:49.731798Z",
     "iopub.status.busy": "2024-09-10T17:15:49.731439Z",
     "iopub.status.idle": "2024-09-10T17:15:49.754226Z",
     "shell.execute_reply": "2024-09-10T17:15:49.753240Z"
    },
    "papermill": {
     "duration": 0.031746,
     "end_time": "2024-09-10T17:15:49.756697",
     "exception": false,
     "start_time": "2024-09-10T17:15:49.724951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.885330e+05\n",
       "mean     4.387802e+04\n",
       "std      7.881952e+04\n",
       "min      2.000000e+03\n",
       "25%      1.700000e+04\n",
       "50%      3.082500e+04\n",
       "75%      4.990000e+04\n",
       "max      2.954083e+06\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'price'\n",
    "train[label].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad80f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T17:15:49.771164Z",
     "iopub.status.busy": "2024-09-10T17:15:49.770812Z",
     "iopub.status.idle": "2024-09-11T04:56:52.034250Z",
     "shell.execute_reply": "2024-09-11T04:56:52.033079Z"
    },
    "papermill": {
     "duration": 42062.273366,
     "end_time": "2024-09-11T04:56:52.036624",
     "exception": false,
     "start_time": "2024-09-10T17:15:49.763258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240910_171549\"\n",
      "Verbosity: 3 (Detailed Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "GPU Count:          2\n",
      "Memory Avail:       30.16 GB / 31.36 GB (96.2%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'ag_args_fit': {'num_gpus': 1},\n",
      " 'auto_stack': True,\n",
      " 'excluded_model_types': ['KNN'],\n",
      " 'num_bag_sets': 1,\n",
      " 'verbosity': 3}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': {'num_gpus': 1},\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'enable_ray_logging': True,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': ['KNN'],\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': 1,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9900s of the 39600s of remaining time (25%).\n",
      "2024-09-10 17:15:56,124\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-09-10 17:15:59,538\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/learner.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/predictor.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Beginning AutoGluon training ... Time limit = 9894s\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Train Data Rows:    167584\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Train Data Columns: 12\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Label Column:       price\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tAvailable Memory:                    30264.46 MB\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTrain Data (Original)  Memory Usage: 103.70 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int64', 'int')     : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', 'object') : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])       : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool']) : 1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])       : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool']) : 1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t0.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t4 features in original data used to generate 4 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t0.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t0.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting TextSpecialFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tFitting BinnedFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('float', ['text_special']) : 7 | ['engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', 'engine.special_ratio', 'engine.symbol_ratio..', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('int', ['text_special'])   : 5 | ['engine.char_count', 'engine.word_count', 'engine.symbol_count..', 'engine.symbol_count. ', 'engine.symbol_count./']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('int', ['binned', 'text_special']) : 12 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t0.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t1 duplicate columns removed: ['engine.symbol_count. ']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t0.5s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t11 features in original data used to generate 11 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t5.7s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t1 features in original data used to generate 11 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting TextNgramFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tFitting CountVectorizer for text features: ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\tCountVectorizer(dtype=<class 'numpy.uint8'>, max_features=10000, min_df=30,\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m                 ngram_range=(1, 3))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tCountVectorizer fit with vocabulary size = 116\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['text_ngram']) : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t5.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t1 features in original data used to generate 117 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', [])                    :   7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', ['text_as_category'])  :   1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])                         :   3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['binned', 'text_special']) :  11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])                   :   1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['text_ngram'])             : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', [])                    :   7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', ['text_as_category'])  :   1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])                         :   3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['binned', 'text_special']) :  11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])                   :   1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['text_ngram'])             : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t0.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t140 features in original data used to generate 140 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t54 duplicate columns removed: ['__nlp__.0l cylinder engine', '__nlp__.0l straight', '__nlp__.0l straight cylinder', '__nlp__.0l v6 cylinder', '__nlp__.2l cylinder', '__nlp__.2l cylinder engine', '__nlp__.3l cylinder', '__nlp__.3l cylinder engine', '__nlp__.4l cylinder', '__nlp__.4l cylinder engine', '__nlp__.7l cylinder', '__nlp__.7l cylinder engine', '__nlp__.5l cylinder', '__nlp__.5l cylinder engine', '__nlp__.5l v6 cylinder', '__nlp__.6l cylinder', '__nlp__.6l cylinder engine', '__nlp__.6l v6 cylinder', '__nlp__.8l', '__nlp__.8l cylinder', '__nlp__.8l cylinder engine', '__nlp__.0hp electric motor', '__nlp__.flat cylinder', '__nlp__.flat cylinder engine', '__nlp__.cylinder engine flex', '__nlp__.engine flex', '__nlp__.engine flex fuel', '__nlp__.flex', '__nlp__.flex fuel', '__nlp__.flex fuel capability', '__nlp__.fuel capability', '__nlp__.engine', '__nlp__.diesel fuel', '__nlp__.engine diesel', '__nlp__.engine diesel fuel', '__nlp__.engine gas', '__nlp__.engine gas electric', '__nlp__.gas electric', '__nlp__.gas electric hybrid', '__nlp__.engine gasoline', '__nlp__.gasoline', '__nlp__.electric fuel system', '__nlp__.electric motor', '__nlp__.electric motor electric', '__nlp__.fuel system', '__nlp__.motor electric', '__nlp__.motor electric fuel', '__nlp__.system', '__nlp__.gasoline fuel', '__nlp__.i4 16v', '__nlp__.straight cylinder', '__nlp__.straight cylinder engine', '__nlp__.twin turbo', '__nlp__.v6 cylinder engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['text_ngram'])             : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t\t('int', ['text_ngram'])             : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t6.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t\t86 features in original data used to generate 86 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int64', 'int')     : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('object', 'object') : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('category', 'category') :  8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int64', 'int')         :  3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int8', 'int')          :  1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('uint16', 'int')        : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('uint8', 'int')         : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t\t('int', ['text_ngram'])             : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t19.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t12 features in original data used to generate 86 features in processed data.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTrain Data (Processed) Memory Usage: 27.81 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Data preprocessing and feature engineering runtime = 19.74s ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/learner.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}, {'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17}}, {'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19}}, {'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26}}, {'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35}}, {'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38}}, {'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41}}, {'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49}}, {'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52}}, {'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59}}, {'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60}}, {'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65}}, {'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77}}, {'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79}}, {'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84}}, {'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87}}, {'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92}}, {'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96}}, {'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge', {'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3}}, {'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6}}, {'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14}}, {'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18}}, {'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27}}, {'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31}}, {'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37}}, {'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44}}, {'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48}}, {'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56}}, {'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69}}, {'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74}}, {'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10}}, {'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12}}, {'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24}}, {'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33}}, {'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67}}, {'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72}}, {'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89}}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}, {'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'name_suffix': '_r194', 'priority': -22}}, {'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'name_suffix': '_r98', 'priority': -36}}, {'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'name_suffix': '_r49', 'priority': -57}}, {'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'name_suffix': '_r31', 'priority': -64}}, {'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'name_suffix': '_r22', 'priority': -70}}, {'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'name_suffix': '_r95', 'priority': -93}}, {'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'name_suffix': '_r34', 'priority': -94}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}, {'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15}}, {'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21}}, {'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25}}, {'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28}}, {'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30}}, {'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34}}, {'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40}}, {'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46}}, {'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51}}, {'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54}}, {'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55}}, {'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66}}, {'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71}}, {'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73}}, {'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75}}, {'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80}}, {'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82}}, {'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85}}, {'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88}}, {'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13}}, {'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32}}, {'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45}}, {'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47}}, {'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63}}, {'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68}}, {'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9}}, {'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23}}, {'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43}}, {'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53}}, {'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62}}, {'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78}}, {'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/utils/data/X.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/utils/data/y.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Model configs that will be trained (in order):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_BAG_L1: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r177_BAG_L1: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r79_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r131_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r191_BAG_L1: \t{'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r9_BAG_L1: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r96_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r22_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r33_BAG_L1: \t{'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r33', 'priority': -8, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r42_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r137_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r102_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r13_BAG_L1: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r195_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r188_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r145_BAG_L1: \t{'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r89_BAG_L1: \t{'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r89', 'priority': -16, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r30_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r130_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r86_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r50_BAG_L1: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r11_BAG_L1: \t{'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r194_BAG_L1: \t{'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r194', 'priority': -22, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r172_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r69_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r103_BAG_L1: \t{'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r14_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r161_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r143_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r70_BAG_L1: \t{'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r156_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r196_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r39_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r167_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r95_BAG_L1: \t{'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r41_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r98_BAG_L1: \t{'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r98', 'priority': -36, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r15_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r158_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r86_BAG_L1: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r37_BAG_L1: \t{'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r197_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r49_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r49_BAG_L1: \t{'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r143_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r127_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r134_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r34_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r94_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r143_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r128_BAG_L1: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r111_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r31_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r4_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r65_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r88_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r30_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r49_BAG_L1: \t{'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r49', 'priority': -57, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r5_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r87_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r71_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r143_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r178_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r166_BAG_L1: \t{'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r31_BAG_L1: \t{'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r31', 'priority': -64, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r185_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r160_BAG_L1: \t{'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r60_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r15_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r135_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r22_BAG_L1: \t{'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r22', 'priority': -70, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r69_BAG_L1: \t{'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r6_BAG_L1: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r138_BAG_L1: \t{'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r121_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r172_BAG_L1: \t{'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r180_BAG_L1: \t{'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r76_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r197_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r121_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r127_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r16_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r194_BAG_L1: \t{'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r12_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r135_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r4_BAG_L1: \t{'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r126_BAG_L1: \t{'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r36_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r100_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r163_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r198_BAG_L1: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r187_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r19_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r95_BAG_L1: \t{'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r95', 'priority': -93, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r34_BAG_L1: \t{'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r34', 'priority': -94, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r42_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r1_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r89_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting 106 L1 models ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6581.5s of the 9874.7s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBMXT_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "\u001b[36m(_ray_fit pid=436)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=436)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=436)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=436)\u001b[0m [50]\tvalid_set's rmse: 82322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=493)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=437)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=437)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=493)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=493)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=493)\u001b[0m [50]\tvalid_set's rmse: 69880.3\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=551)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=520)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=520)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=551)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=551)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=551)\u001b[0m [50]\tvalid_set's rmse: 67583\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=609)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=578)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=578)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=609)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=609)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=609)\u001b[0m [50]\tvalid_set's rmse: 61900.6\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73332.5258\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t40.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.31s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t16039.5\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 6533.8s of the 9827.0s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=637)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=637)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=637)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "\u001b[36m(_ray_fit pid=759)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=759)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=759)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=759)\u001b[0m [50]\tvalid_set's rmse: 82343.8\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=816)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=760)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=760)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=816)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=816)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=816)\u001b[0m [50]\tvalid_set's rmse: 70321.1\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=875)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=845)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=845)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=875)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=875)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=875)\u001b[0m [50]\tvalid_set's rmse: 67767.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=933)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=903)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=903)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=933)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=933)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=933)\u001b[0m [50]\tvalid_set's rmse: 62461.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73655.3912\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t37.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.77s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t27063.2\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 6493.56s of the 9786.76s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=962)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=962)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=962)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting RandomForestMSE_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t34.41s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-77530.8192\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t328.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t11.55s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t14503.4\t = Inference  throughput (rows/s | 167584 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 6152.55s of the 9445.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.72%)\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m 0:\tlearn: 77311.6402447\ttest: 88003.1367496\tbest: 88003.1367496 (0)\ttotal: 268ms\tremaining: 536ms\n",
      "\u001b[36m(_ray_fit pid=962)\u001b[0m [50]\tvalid_set's rmse: 71115.9\n",
      "\u001b[36m(_ray_fit pid=1098)\u001b[0m bestTest = 86222.31329\n",
      "\u001b[36m(_ray_fit pid=1098)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1098)\u001b[0m 120:\tlearn: 71345.6164832\ttest: 82274.6601930\tbest: 82274.6601930 (120)\ttotal: 3.39s\tremaining: 43.9s\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m bestTest = 87325.04992\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m bestTest = 83150.38293\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m bestIteration = 289\n",
      "\u001b[36m(_ray_fit pid=1099)\u001b[0m Shrink model to first 290 iterations.\n",
      "\u001b[36m(_ray_fit pid=1098)\u001b[0m 280:\tlearn: 70368.5066108\ttest: 82201.4294370\tbest: 82201.4294370 (280)\ttotal: 8.25s\tremaining: 41.3s\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m bestTest = 74295.31455\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1098)\u001b[0m Shrink model to first 386 iterations.\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m 2:\tlearn: 78458.3309823\ttest: 74295.3145535\tbest: 74295.3145535 (2)\ttotal: 41.7ms\tremaining: 0us\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m bestTest = 83321.07326\n",
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m 140:\tlearn: 73100.1212964\ttest: 69576.9705613\tbest: 69572.4350924 (137)\ttotal: 3.36s\tremaining: 51.3s\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m bestTest = 69562.80775\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m bestIteration = 201\n",
      "\u001b[36m(_ray_fit pid=1181)\u001b[0m Shrink model to first 202 iterations.\n",
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m 200:\tlearn: 71302.9849077\ttest: 79090.0991733\tbest: 79083.0913919 (164)\ttotal: 5.36s\tremaining: 1m 8s\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m bestTest = 79075.50062\n",
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m bestIteration = 293\n",
      "\u001b[36m(_ray_fit pid=1221)\u001b[0m Shrink model to first 294 iterations.\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m 0:\tlearn: 79420.2177508\ttest: 73582.9002673\tbest: 73582.9002673 (0)\ttotal: 25.7ms\tremaining: 1m 1s\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m bestTest = 72776.19388\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m bestTest = 74387.68963\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m 220:\tlearn: 72901.1554674\ttest: 67401.5061287\tbest: 67400.1366570 (211)\ttotal: 5.05s\tremaining: 49.8s\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m bestTest = 67365.69762\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m bestIteration = 339\n",
      "\u001b[36m(_ray_fit pid=1271)\u001b[0m Shrink model to first 340 iterations.\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m 180:\tlearn: 72877.1736096\ttest: 69477.0382777\tbest: 69477.0382777 (180)\ttotal: 4.82s\tremaining: 1m 3s\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m bestTest = 67422.59426\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m 2:\tlearn: 79393.5474715\ttest: 67422.5942627\tbest: 67422.5942627 (2)\ttotal: 36.1ms\tremaining: 0us\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m Shrink model to first 330 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m bestTest = 69446.73157\n",
      "\u001b[36m(_ray_fit pid=1316)\u001b[0m bestIteration = 329\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m 140:\tlearn: 74258.1946045\ttest: 61776.0344826\tbest: 61776.0344826 (140)\ttotal: 3.44s\tremaining: 1m 4s\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m bestTest = 75739.79947\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m Shrink model to first 258 iterations.\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m 120:\tlearn: 73078.4686743\ttest: 70804.7804898\tbest: 70804.2884440 (119)\ttotal: 2.83s\tremaining: 52.6s\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m bestTest = 61651.18291\n",
      "\u001b[36m(_ray_fit pid=1361)\u001b[0m bestIteration = 257\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m bestTest = 70751.41319\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m bestIteration = 203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73252.3851\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t64.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t18807.3\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6085.48s of the 9378.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting ExtraTreesMSE_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t34.59s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-76550.3532\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t206.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t11.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t14960.8\t = Inference  throughput (rows/s | 167584 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5866.21s of the 9159.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetFastAI_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.08%)\n",
      "\u001b[36m(_ray_fit pid=1562)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=1562)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1562)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1641)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1641)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1641)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1721)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=1721)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1721)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1801)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=1801)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1801)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1830)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=1830)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1830)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73559.9855\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t796.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t3.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t5593.4\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 5066.24s of the 8359.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting XGBoost_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.95%)\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:44] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m   warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m [0]\tvalidation_0-rmse:86424.94535\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m Shrink model to first 204 iterations.\n",
      "\u001b[36m(_ray_fit pid=1405)\u001b[0m 240:\tlearn: 72295.6618773\ttest: 70754.5600447\tbest: 70751.4131852 (203)\ttotal: 5.42s\tremaining: 47.8s\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1972)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:50] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1973)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1973)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1973)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1973)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1973)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m [0]\tvalidation_0-rmse:74586.04403\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:56] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2035)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m [0]\tvalidation_0-rmse:74606.25773\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2097)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:02] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2095)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2095)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2095)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2095)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2095)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m [0]\tvalidation_0-rmse:67650.73027\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74076.0485\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t22.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t18556.4\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5040.98s of the 8334.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetTorch_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\u001b[36m(_ray_fit pid=2313)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2313)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:03] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:42:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2312)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2312)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2381)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2381)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2416)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2416)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2451)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2451)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2486)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2486)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2556)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2556)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2521)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2521)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73822.7902\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t708.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.67s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t7859.8\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4330.0s of the 7623.2s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBMLarge_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.86%)\n",
      "\u001b[36m(_ray_fit pid=2683)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2683)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2683)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2683)\u001b[0m [50]\tvalid_set's rmse: 84303.2\n",
      "\u001b[36m(_ray_fit pid=2185)\u001b[0m [89]\tvalidation_0-rmse:71544.70260\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2739)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2682)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2682)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=2739)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2739)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2739)\u001b[0m [50]\tvalid_set's rmse: 70767\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2798)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2741)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2741)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=2798)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2798)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2798)\u001b[0m [50]\tvalid_set's rmse: 68297.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2856)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2800)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2800)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=2856)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2856)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2856)\u001b[0m [50]\tvalid_set's rmse: 62681.5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74049.3777\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t47.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t16313.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 4279.87s of the 7573.06s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=2884)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2884)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=2884)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_r177_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m 0:\tlearn: 77276.1350443\ttest: 86726.5606146\tbest: 86726.5606146 (0)\ttotal: 50.2ms\tremaining: 100ms\n",
      "\u001b[36m(_ray_fit pid=2884)\u001b[0m [100]\tvalid_set's rmse: 71602.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m 2:\tlearn: 76185.8383622\ttest: 85792.1180353\tbest: 85792.1180353 (2)\ttotal: 125ms\tremaining: 0us\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m bestTest = 85792.11804\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m 140:\tlearn: 70691.4064705\ttest: 82244.6628969\tbest: 82244.6628969 (140)\ttotal: 3.04s\tremaining: 35.4s\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m bestTest = 86955.38217\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m bestTest = 83157.20205\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m bestIteration = 150\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m Shrink model to first 151 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m 0:\tlearn: 79051.7437160\ttest: 74884.6925115\tbest: 74884.6925115 (0)\ttotal: 44.3ms\tremaining: 88.7ms\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m bestTest = 82224.65375\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m bestIteration = 209\n",
      "\u001b[36m(_ray_fit pid=3006)\u001b[0m Shrink model to first 210 iterations.\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m bestTest = 73786.16406\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m 60:\tlearn: 72022.5075957\ttest: 79173.1275396\tbest: 79173.1275396 (60)\ttotal: 1.69s\tremaining: 39.9s\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m bestTest = 82900.02869\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m bestTest = 69591.72606\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m bestIteration = 173\n",
      "\u001b[36m(_ray_fit pid=3089)\u001b[0m Shrink model to first 174 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m 0:\tlearn: 79253.8800376\ttest: 73405.0072275\tbest: 73405.0072275 (0)\ttotal: 39.4ms\tremaining: 78.9ms\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m bestTest = 79094.099\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m bestIteration = 113\n",
      "\u001b[36m(_ray_fit pid=3123)\u001b[0m Shrink model to first 114 iterations.\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m bestTest = 72258.76715\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m 140:\tlearn: 72805.6678050\ttest: 67398.7998356\tbest: 67378.8394241 (130)\ttotal: 3.09s\tremaining: 36.3s\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3189)\u001b[0m bestTest = 73883.91839\n",
      "\u001b[36m(_ray_fit pid=3189)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m bestTest = 67363.01961\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m bestIteration = 186\n",
      "\u001b[36m(_ray_fit pid=3184)\u001b[0m Shrink model to first 187 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m 2:\tlearn: 78947.4033487\ttest: 66871.7642540\tbest: 66871.7642540 (2)\ttotal: 99.6ms\tremaining: 0us\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m bestTest = 66871.76425\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3189)\u001b[0m Shrink model to first 195 iterations.\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m 160:\tlearn: 72274.6864176\ttest: 70728.5886051\tbest: 70723.6598612 (156)\ttotal: 3.61s\tremaining: 36.7s\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m bestTest = 75268.68563\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m bestTest = 70712.27516\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m bestIteration = 170\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m Shrink model to first 171 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73245.1053\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t49.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.98s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t21360.9\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 4227.84s of the 7521.04s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3280)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetTorch_r79_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\u001b[36m(_ray_fit pid=3460)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3460)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3461)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3461)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3529)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3529)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3564)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3564)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3599)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3599)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3662)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3662)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3634)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3634)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3704)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3704)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73874.985\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t706.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.7s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t7753.1\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 3518.06s of the 6811.25s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_r131_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "\u001b[36m(_ray_fit pid=3830)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3830)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3830)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3830)\u001b[0m [50]\tvalid_set's rmse: 83877.3\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m 320:\tlearn: 71982.3782355\ttest: 61542.3343293\tbest: 61524.4833450 (281)\ttotal: 7.23s\tremaining: 32.6s\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m bestTest = 61524.48334\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m bestIteration = 281\n",
      "\u001b[36m(_ray_fit pid=3278)\u001b[0m Shrink model to first 282 iterations.\n",
      "\u001b[36m(_ray_fit pid=3830)\u001b[0m [200]\tvalid_set's rmse: 82188.3\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3888)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3831)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3831)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=3888)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3888)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3888)\u001b[0m [50]\tvalid_set's rmse: 71615.6\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3888)\u001b[0m [200]\tvalid_set's rmse: 70214.8\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3946)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3916)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3916)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=3946)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3946)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3946)\u001b[0m [50]\tvalid_set's rmse: 69739.8\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3974)\u001b[0m [100]\tvalid_set's rmse: 70193.9\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3974)\u001b[0m [250]\tvalid_set's rmse: 69748.9\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4004)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3974)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=3974)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4004)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=4004)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4004)\u001b[0m [50]\tvalid_set's rmse: 64231.7\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m [50]\tvalid_set's rmse: 72952.7\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m [200]\tvalid_set's rmse: 71038.3\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73543.3012\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t72.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t3.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t6491.1\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 3442.27s of the 6735.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetFastAI_r191_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.09%)\n",
      "\u001b[36m(_ray_fit pid=4154)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=4154)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4154)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4153)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=4153)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4153)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4232)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=4232)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4232)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4272)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=4272)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4272)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4353)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=4353)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4353)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4312)\u001b[0m No improvement since epoch 5: early stopping\n",
      "\u001b[36m(_ray_fit pid=4312)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4312)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4393)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=4393)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4393)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4433)\u001b[0m No improvement since epoch 6: early stopping\n",
      "\u001b[36m(_ray_fit pid=4433)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4433)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73543.9785\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t941.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t3.87s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t5413.3\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 2497.65s of the 5790.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_r9_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.08%)\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m 0:\tlearn: 77544.3546225\ttest: 86992.5199819\tbest: 86992.5199819 (0)\ttotal: 202ms\tremaining: 404ms\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m 2:\tlearn: 76837.8078461\ttest: 86460.9734512\tbest: 86460.9734512 (2)\ttotal: 261ms\tremaining: 0us\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m bestTest = 86460.97345\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=4032)\u001b[0m [250]\tvalid_set's rmse: 71113.4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m Shrink model to first 116 iterations.\n",
      "\u001b[36m(_ray_fit pid=4565)\u001b[0m 160:\tlearn: 65400.9823889\ttest: 83184.6799520\tbest: 83175.4654902 (149)\ttotal: 3.3s\tremaining: 14.7s\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m bestTest = 82014.14335\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4564)\u001b[0m bestIteration = 115\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4565)\u001b[0m Shrink model to first 195 iterations.\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m 0:\tlearn: 79278.8770445\ttest: 75185.7129808\tbest: 75185.7129808 (0)\ttotal: 18.2ms\tremaining: 23.3s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m bestTest = 74569.56452\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m Shrink model to first 116 iterations.\n",
      "\u001b[36m(_ray_fit pid=4681)\u001b[0m 140:\tlearn: 67166.0855009\ttest: 79026.0921268\tbest: 79013.5850346 (124)\ttotal: 2.42s\tremaining: 14.1s\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m bestTest = 69758.78758\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m bestIteration = 115\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4681)\u001b[0m Shrink model to first 125 iterations.\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m 20:\tlearn: 74707.4982962\ttest: 69480.3437166\tbest: 69480.3437166 (20)\ttotal: 361ms\tremaining: 16.8s\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m bestTest = 73032.38932\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m Shrink model to first 130 iterations.\n",
      "\u001b[36m(_ray_fit pid=4771)\u001b[0m 160:\tlearn: 67857.6282080\ttest: 69550.7148467\tbest: 69525.6789298 (126)\ttotal: 2.87s\tremaining: 13.9s\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m bestTest = 67270.97229\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4737)\u001b[0m bestIteration = 129\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4771)\u001b[0m Shrink model to first 127 iterations.\n",
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m 40:\tlearn: 73366.2196592\ttest: 62429.3251531\tbest: 62429.3251531 (40)\ttotal: 760ms\tremaining: 17.8s\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m bestTest = 67657.42082\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4827)\u001b[0m Shrink model to first 181 iterations.\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m 180:\tlearn: 67204.7699186\ttest: 70799.9220948\tbest: 70760.3100595 (142)\ttotal: 2.65s\tremaining: 10.7s\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m bestTest = 70760.31006\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m bestIteration = 142\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73237.922\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t39.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t20637.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 2455.7s of the 5748.89s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_r96_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.65%)\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m [50]\tvalid_set's rmse: 85254.5\n",
      "\u001b[36m(_ray_fit pid=4861)\u001b[0m Shrink model to first 143 iterations.\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m [250]\tvalid_set's rmse: 83245.2\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m [450]\tvalid_set's rmse: 83210.1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5072)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5015)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5015)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=5072)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5072)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5072)\u001b[0m [50]\tvalid_set's rmse: 71868.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5100)\u001b[0m [150]\tvalid_set's rmse: 79291.1\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5100)\u001b[0m [350]\tvalid_set's rmse: 79011.3\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5100)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5100)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m [100]\tvalid_set's rmse: 68480.6\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5159)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5159)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5159)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m [300]\tvalid_set's rmse: 67458.3\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5130)\u001b[0m [500]\tvalid_set's rmse: 67440.8\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5188)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5159)\u001b[0m [400]\tvalid_set's rmse: 69293.5\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5188)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5188)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5188)\u001b[0m [150]\tvalid_set's rmse: 62102.3\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5217)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5217)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=5217)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5188)\u001b[0m [350]\tvalid_set's rmse: 61687\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5217)\u001b[0m [200]\tvalid_set's rmse: 71019.9\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5217)\u001b[0m [400]\tvalid_set's rmse: 70915.3\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73278.1604\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t85.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t5.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t4017.2\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 2367.38s of the 5660.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetTorch_r22_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\u001b[36m(_ray_fit pid=5337)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5337)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5338)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5338)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5406)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5406)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5441)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5441)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5476)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5476)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5511)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5511)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5546)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5546)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=5582)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5582)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-90504.6408\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t294.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t8464.5\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 2070.12s of the 5363.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting XGBoost_r33_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.73%)\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:40] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m   warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m [0]\tvalidation_0-rmse:88224.60899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5708)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:48] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5709)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m [0]\tvalidation_0-rmse:75335.11470\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5769)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:56] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5771)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5771)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5771)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5771)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5771)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m [0]\tvalidation_0-rmse:73863.31803\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5831)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:05] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5833)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5833)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5833)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5833)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5833)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m [0]\tvalidation_0-rmse:68523.75215\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5893)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74174.9112\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t31.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t16628.4\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 2036.18s of the 5329.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting ExtraTrees_r42_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t31.63s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:05] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:32:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-76224.6932\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t160.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t11.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t15059.4\t = Inference  throughput (rows/s | 167584 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1862.9s of the 5156.1s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_r137_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.65%)\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m 0:\tlearn: 77354.9331597\ttest: 88031.6631084\tbest: 88031.6631084 (0)\ttotal: 33.5ms\tremaining: 67ms\n",
      "\u001b[36m(_ray_fit pid=5920)\u001b[0m [182]\tvalidation_0-rmse:71672.58072\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m 2:\tlearn: 76632.4578642\ttest: 87395.1376023\tbest: 87395.1376023 (2)\ttotal: 81.9ms\tremaining: 0us\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m bestTest = 87395.1376\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m 200:\tlearn: 71567.0556539\ttest: 82257.5063112\tbest: 82254.5609745 (196)\ttotal: 2.98s\tremaining: 5.49s\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m bestTest = 86266.64643\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m bestTest = 82239.71585\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m bestIteration = 317\n",
      "\u001b[36m(_ray_fit pid=6060)\u001b[0m Shrink model to first 318 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m 0:\tlearn: 79247.2447927\ttest: 75092.4158096\tbest: 75092.4158096 (0)\ttotal: 37ms\tremaining: 74ms\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m bestTest = 83147.66668\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m bestIteration = 398\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m Shrink model to first 399 iterations.\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m bestTest = 74355.43684\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m 200:\tlearn: 72121.1453271\ttest: 79136.2646294\tbest: 79134.5997904 (194)\ttotal: 2.9s\tremaining: 8.21s\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m bestTest = 83407.4181\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m bestTest = 69595.02893\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m bestIteration = 358\n",
      "\u001b[36m(_ray_fit pid=6143)\u001b[0m Shrink model to first 359 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m 0:\tlearn: 79460.5505184\ttest: 73635.7218546\tbest: 73635.7218546 (0)\ttotal: 32ms\tremaining: 64.1ms\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m bestTest = 79119.74373\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m bestIteration = 329\n",
      "\u001b[36m(_ray_fit pid=6175)\u001b[0m Shrink model to first 330 iterations.\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m bestTest = 72856.799\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m 240:\tlearn: 73636.8413477\ttest: 67459.4856027\tbest: 67459.2451885 (228)\ttotal: 3.52s\tremaining: 7.54s\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m bestTest = 74393.75364\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m bestTest = 69428.15014\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m bestIteration = 452\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m Shrink model to first 453 iterations.\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m 560:\tlearn: 72852.6184845\ttest: 67386.5206336\tbest: 67378.6343818 (520)\ttotal: 8.59s\tremaining: 3s\u001b[32m [repeated 29x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m bestTest = 67474.06319\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m Shrink model to first 521 iterations.\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m 0:\tlearn: 80142.1883749\ttest: 68289.1478176\tbest: 68289.1478176 (0)\ttotal: 19.1ms\tremaining: 15.4s\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m bestTest = 75830.73178\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m 360:\tlearn: 74225.9826592\ttest: 61708.9324439\tbest: 61708.0596240 (358)\ttotal: 5.29s\tremaining: 6.51s\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m bestTest = 61698.8894\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m bestIteration = 379\n",
      "\u001b[36m(_ray_fit pid=6326)\u001b[0m Shrink model to first 380 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73275.4024\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t53.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t20648.9\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 1807.31s of the 5100.5s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetFastAI_r102_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.10%)\n",
      "\u001b[36m(_ray_fit pid=6513)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=6513)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6513)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6514)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=6514)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6514)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6622)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6622)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6673)\u001b[0m No improvement since epoch 5: early stopping\n",
      "\u001b[36m(_ray_fit pid=6673)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6673)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6713)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=6713)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6713)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6753)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=6753)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6753)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6793)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=6793)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6793)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73715.4575\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t150.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t21904.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1654.25s of the 4947.45s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_r13_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.08%)\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m 0:\tlearn: 77577.3832646\ttest: 88238.9771801\tbest: 88238.9771801 (0)\ttotal: 68.7ms\tremaining: 137ms\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m bestTest = 70760.65527\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m bestIteration = 466\n",
      "\u001b[36m(_ray_fit pid=6360)\u001b[0m Shrink model to first 467 iterations.\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m bestTest = 87958.65449\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m 100:\tlearn: 72034.0747586\ttest: 82637.3401578\tbest: 82637.3401578 (100)\ttotal: 3.08s\tremaining: 12.9s\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m bestTest = 86859.21794\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m 240:\tlearn: 70886.5451003\ttest: 83210.9168015\tbest: 83210.9168015 (240)\ttotal: 8.58s\tremaining: 15.5s\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m 420:\tlearn: 70249.4433323\ttest: 82225.2722604\tbest: 82225.2722604 (420)\ttotal: 13.6s\tremaining: 3.37s\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m bestTest = 82214.35754\n",
      "\u001b[36m(_ray_fit pid=6924)\u001b[0m bestIteration = 524\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m 560:\tlearn: 69690.6578350\ttest: 83091.8291028\tbest: 83089.6651838 (551)\ttotal: 19.7s\tremaining: 4.06s\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m bestTest = 83076.95524\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m bestIteration = 672\n",
      "\u001b[36m(_ray_fit pid=6925)\u001b[0m Shrink model to first 673 iterations.\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m 0:\tlearn: 79495.4283907\ttest: 75358.3424905\tbest: 75358.3424905 (0)\ttotal: 135ms\tremaining: 1m 27s\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m bestTest = 75022.47355\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m bestIteration = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m bestTest = 84001.53416\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m 180:\tlearn: 73195.0841449\ttest: 69678.6388418\tbest: 69678.6388418 (180)\ttotal: 5.85s\tremaining: 15.3s\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m 140:\tlearn: 72079.6645227\ttest: 79254.7072387\tbest: 79254.7072387 (140)\ttotal: 4.89s\tremaining: 21s\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m bestTest = 69553.73755\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m bestIteration = 429\n",
      "\u001b[36m(_ray_fit pid=7007)\u001b[0m Shrink model to first 430 iterations.\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m 300:\tlearn: 71197.6056049\ttest: 79050.0137748\tbest: 79050.0137748 (300)\ttotal: 10.3s\tremaining: 15.3s\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m 460:\tlearn: 70554.2058115\ttest: 79029.0361747\tbest: 79025.9249056 (433)\ttotal: 16.1s\tremaining: 9.97s\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m bestTest = 73535.57917\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m Shrink model to first 434 iterations.\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m 100:\tlearn: 74223.3282721\ttest: 68067.3739358\tbest: 68067.3739358 (100)\ttotal: 3.57s\tremaining: 24s\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m bestTest = 79025.92491\n",
      "\u001b[36m(_ray_fit pid=7052)\u001b[0m bestIteration = 433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m bestTest = 75085.16825\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m 260:\tlearn: 73066.6628509\ttest: 67489.3615279\tbest: 67489.3615279 (260)\ttotal: 8.79s\tremaining: 17.5s\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m 220:\tlearn: 73005.8122539\ttest: 69572.3775338\tbest: 69572.3775338 (220)\ttotal: 6.75s\tremaining: 13.8s\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m 560:\tlearn: 71887.7583105\ttest: 67347.2941975\tbest: 67347.2941975 (560)\ttotal: 19s\tremaining: 7.46s\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m bestTest = 67340.76811\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m bestIteration = 634\n",
      "\u001b[36m(_ray_fit pid=7097)\u001b[0m Shrink model to first 635 iterations.\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m 520:\tlearn: 71905.7121036\ttest: 69410.2906701\tbest: 69410.0223969 (519)\ttotal: 17s\tremaining: 4.97s\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m 660:\tlearn: 71461.0058558\ttest: 69389.9566620\tbest: 69389.7460204 (659)\ttotal: 22s\tremaining: 399ms\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m bestTest = 69388.86305\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m bestIteration = 668\n",
      "\u001b[36m(_ray_fit pid=7142)\u001b[0m Shrink model to first 669 iterations.\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m 80:\tlearn: 75292.4653478\ttest: 62695.1540613\tbest: 62695.1540613 (80)\ttotal: 2.71s\tremaining: 20.2s\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m bestTest = 68174.62712\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m bestIteration = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m bestTest = 76515.12803\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m 240:\tlearn: 74022.4663293\ttest: 61802.8652656\tbest: 61802.8652656 (240)\ttotal: 7.73s\tremaining: 14.2s\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m 400:\tlearn: 73378.1893433\ttest: 61659.9283418\tbest: 61658.2657328 (396)\ttotal: 13s\tremaining: 9.14s\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m 380:\tlearn: 72258.5533954\ttest: 70711.0519245\tbest: 70710.8933554 (378)\ttotal: 12s\tremaining: 10.8s\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m bestTest = 70701.62777\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m bestIteration = 427\n",
      "\u001b[36m(_ray_fit pid=7232)\u001b[0m Shrink model to first 428 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73214.9673\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t109.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t9703.2\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 1541.71s of the 4834.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting RandomForest_r195_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t37.26s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-76625.831\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t310.85s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t14.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t11775.8\t = Inference  throughput (rows/s | 167584 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 1213.9s of the 4507.1s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_r188_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.85%)\n",
      "\u001b[36m(_ray_fit pid=7389)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7389)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7389)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7388)\u001b[0m [50]\tvalid_set's rmse: 82529.8\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m 683:\tlearn: 72459.7890478\ttest: 61573.9139175\tbest: 61573.9139175 (683)\ttotal: 22.6s\tremaining: 0us\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m bestTest = 61573.91392\n",
      "\u001b[36m(_ray_fit pid=7187)\u001b[0m bestIteration = 683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7446)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7388)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7388)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=7446)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7446)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7446)\u001b[0m [50]\tvalid_set's rmse: 70087.2\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7503)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7471)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7471)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=7503)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7503)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7503)\u001b[0m [50]\tvalid_set's rmse: 68013.4\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7561)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7531)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7531)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=7561)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7561)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7561)\u001b[0m [50]\tvalid_set's rmse: 62160\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73463.1388\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t49.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t8763.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 1161.47s of the 4454.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetFastAI_r145_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.11%)\n",
      "\u001b[36m(_ray_fit pid=7588)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7588)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=7588)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=7710)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7710)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7710)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7791)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 12)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7791)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7791)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7873)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7873)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7873)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7954)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7954)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7954)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73488.8585\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t912.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t6.82s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t3069.5\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 245.7s of the 3538.89s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting XGBoost_r89_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.96%)\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:04] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7982)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7982)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7982)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m [0]\tvalidation_0-rmse:87702.90149\n",
      "\u001b[36m(_ray_fit pid=7588)\u001b[0m [100]\tvalid_set's rmse: 71260.5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8126)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:11] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8125)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8125)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8125)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8125)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8125)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m [0]\tvalidation_0-rmse:74703.42619\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8186)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:16] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m [0]\tvalidation_0-rmse:73201.06943\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8188)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8188)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8188)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8188)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8188)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8250)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8250)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8250)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8250)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8250)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:22] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8248)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m [0]\tvalidation_0-rmse:67830.36905\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8309)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73667.0865\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t21.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t19662.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 221.34s of the 3514.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetTorch_r30_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.62%)\n",
      "\u001b[36m(_ray_fit pid=8463)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "\u001b[36m(_ray_fit pid=8463)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=8463)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:22] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:02:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8534)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 10)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8534)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8534)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8606)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 10)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8606)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8606)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 10)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-78670.4446\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t190.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.97s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t7062.9\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 28.0s of the 3321.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_r130_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.83%)\n",
      "\u001b[36m(_ray_fit pid=8841)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8678)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "\u001b[36m(_ray_fit pid=8678)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=8678)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=8841)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8841)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8841)\u001b[0m [50]\tvalid_set's rmse: 82219.9\n",
      "\u001b[36m(_ray_fit pid=8312)\u001b[0m [158]\tvalidation_0-rmse:71252.73282\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8898)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8842)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8842)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=8898)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8898)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8898)\u001b[0m [50]\tvalid_set's rmse: 69976.6\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8900)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8900)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m [50]\tvalid_set's rmse: 67614.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m \tRan out of time, early stopping on iteration 79. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8958)\u001b[0m \t[60]\tvalid_set's rmse: 67571.2\n",
      "\u001b[36m(_ray_fit pid=8960)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9019)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8960)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=8960)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9017)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9017)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9017)\u001b[0m [50]\tvalid_set's rmse: 62038.3\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9019)\u001b[0m \tRan out of time, early stopping on iteration 80. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=9019)\u001b[0m \t[47]\tvalid_set's rmse: 71038.4\n",
      "\u001b[36m(_ray_fit pid=9017)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73485.8269\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t40.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t17689.3\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r86_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r50_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r11_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r194_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r172_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r69_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r103_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r14_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r161_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r143_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r70_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r156_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r196_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r39_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r167_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r95_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r41_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r98_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r15_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r158_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r86_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r37_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r197_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r49_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r49_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r143_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r127_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r134_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r34_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r94_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r143_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r128_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r111_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r31_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r4_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r65_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r88_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r30_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r49_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r5_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r87_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r71_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r143_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r178_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r166_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r31_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r185_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r160_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r60_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r15_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r135_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r22_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r69_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r6_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r138_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r121_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r172_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r180_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r76_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r197_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r121_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r127_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r16_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r194_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r12_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r135_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r4_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r126_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r36_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r100_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r163_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r198_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r187_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r19_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r95_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r34_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r42_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r1_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r89_BAG_L1 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Model configs that will be trained (in order):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 658.15s of the 3278.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "\u001b[36m(_ray_fit pid=9019)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9019)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Ensemble size: 25\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Ensemble weights: \n",
      "\u001b[36m(_dystack pid=167)\u001b[0m [0.   0.   0.   0.08 0.12 0.   0.   0.16 0.08 0.   0.04 0.12 0.04 0.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m  0.   0.   0.   0.08 0.04 0.04 0.   0.16 0.   0.   0.04]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.62s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.16, 'NeuralNetFastAI_r145_BAG_L1': 0.16, 'NeuralNetFastAI_BAG_L1': 0.12, 'CatBoost_r9_BAG_L1': 0.12, 'ExtraTreesMSE_BAG_L1': 0.08, 'NeuralNetTorch_r79_BAG_L1': 0.08, 'NeuralNetFastAI_r102_BAG_L1': 0.08, 'NeuralNetFastAI_r191_BAG_L1': 0.04, 'LightGBM_r96_BAG_L1': 0.04, 'CatBoost_r13_BAG_L1': 0.04, 'RandomForest_r195_BAG_L1': 0.04, 'LightGBM_r130_BAG_L1': 0.04}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-72969.6729\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t658.1\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Model configs that will be trained (in order):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_BAG_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r177_BAG_L2: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r79_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r131_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r191_BAG_L2: \t{'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r9_BAG_L2: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r96_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r22_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r33_BAG_L2: \t{'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r33', 'priority': -8, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r42_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r137_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r102_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r13_BAG_L2: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r195_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r188_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r145_BAG_L2: \t{'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r89_BAG_L2: \t{'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r89', 'priority': -16, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r30_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r130_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r86_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r50_BAG_L2: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r11_BAG_L2: \t{'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r194_BAG_L2: \t{'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r194', 'priority': -22, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r172_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r69_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r103_BAG_L2: \t{'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r14_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r161_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r143_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r70_BAG_L2: \t{'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r156_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r196_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r39_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r167_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r95_BAG_L2: \t{'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r41_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r98_BAG_L2: \t{'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r98', 'priority': -36, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r15_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r158_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r86_BAG_L2: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r37_BAG_L2: \t{'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r197_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r49_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r49_BAG_L2: \t{'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r143_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r127_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r134_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r34_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r94_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r143_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r128_BAG_L2: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r111_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r31_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r4_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r65_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r88_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r30_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r49_BAG_L2: \t{'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r49', 'priority': -57, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r5_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r87_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r71_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r143_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r178_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r166_BAG_L2: \t{'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r31_BAG_L2: \t{'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r31', 'priority': -64, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r185_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r160_BAG_L2: \t{'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r60_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r15_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r135_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r22_BAG_L2: \t{'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r22', 'priority': -70, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r69_BAG_L2: \t{'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r6_BAG_L2: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r138_BAG_L2: \t{'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r121_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r172_BAG_L2: \t{'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r180_BAG_L2: \t{'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r76_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r197_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r121_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r127_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tRandomForest_r16_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r194_BAG_L2: \t{'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r12_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r135_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r4_BAG_L2: \t{'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tExtraTrees_r126_BAG_L2: \t{'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r36_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r100_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r163_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tCatBoost_r198_BAG_L2: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetFastAI_r187_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r19_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r95_BAG_L2: \t{'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r95', 'priority': -93, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tXGBoost_r34_BAG_L2: \t{'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r34', 'priority': -94, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tLightGBM_r42_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r1_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tNeuralNetTorch_r89_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting 106 L2 models ...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3277.32s of the 3277.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBMXT_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.07%)\n",
      "\u001b[36m(_ray_fit pid=9175)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9175)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9175)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9175)\u001b[0m [50]\tvalid_set's rmse: 63996.4\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9231)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9174)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9174)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9231)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9231)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9231)\u001b[0m [50]\tvalid_set's rmse: 59618.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9289)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9259)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9259)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9289)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9289)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9289)\u001b[0m [50]\tvalid_set's rmse: 73884\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9347)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9317)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9317)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9347)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9347)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9347)\u001b[0m [50]\tvalid_set's rmse: 76407.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73075.8958\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t52.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.76s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t360.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 3221.61s of the 3221.35s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=9374)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9374)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9374)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBM_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.07%)\n",
      "\u001b[36m(_ray_fit pid=9504)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9504)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9504)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9504)\u001b[0m [50]\tvalid_set's rmse: 64445.6\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9560)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9503)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9503)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9560)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9560)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9560)\u001b[0m [50]\tvalid_set's rmse: 60072.8\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9620)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9562)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9562)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9620)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9620)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9620)\u001b[0m [50]\tvalid_set's rmse: 74482.8\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9679)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9622)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9622)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=9679)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9679)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9679)\u001b[0m [50]\tvalid_set's rmse: 76811.2\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73536.591\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t49.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.63s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t361.4\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3169.11s of the 3168.84s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=9706)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9706)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=9706)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting RandomForestMSE_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t34.33s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74823.6175\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1712.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t14.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t354.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1440.84s of the 1440.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting CatBoost_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.14%)\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m 0:\tlearn: 78703.7573224\ttest: 78013.4455484\tbest: 78013.4455484 (0)\ttotal: 21.2ms\tremaining: 42.3ms\n",
      "\u001b[36m(_ray_fit pid=9706)\u001b[0m [50]\tvalid_set's rmse: 82745.9\n",
      "\u001b[36m(_ray_fit pid=9850)\u001b[0m bestTest = 68747.34999\n",
      "\u001b[36m(_ray_fit pid=9850)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m 100:\tlearn: 72135.8054801\ttest: 71759.2722409\tbest: 71759.2722409 (100)\ttotal: 2.78s\tremaining: 4.71s\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m bestTest = 77038.68496\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=9850)\u001b[0m bestTest = 63809.49179\n",
      "\u001b[36m(_ray_fit pid=9850)\u001b[0m bestIteration = 68\n",
      "\u001b[36m(_ray_fit pid=9850)\u001b[0m Shrink model to first 69 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m 2:\tlearn: 79233.2217512\ttest: 65676.2753096\tbest: 65676.2753096 (2)\ttotal: 41.9ms\tremaining: 0us\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m bestTest = 65676.27531\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9849)\u001b[0m Shrink model to first 143 iterations.\n",
      "\u001b[36m(_ray_fit pid=9966)\u001b[0m 20:\tlearn: 72843.3174715\ttest: 79788.8782975\tbest: 79788.8782975 (20)\ttotal: 501ms\tremaining: 8.14s\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9966)\u001b[0m bestTest = 83206.89349\n",
      "\u001b[36m(_ray_fit pid=9966)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m bestTest = 59531.43591\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m bestIteration = 104\n",
      "\u001b[36m(_ray_fit pid=9932)\u001b[0m Shrink model to first 105 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m 2:\tlearn: 77585.1847625\ttest: 78225.9888362\tbest: 78225.9888362 (2)\ttotal: 45.6ms\tremaining: 0us\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m bestTest = 78225.98884\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m bestIteration = 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9966)\u001b[0m Shrink model to first 165 iterations.\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m 120:\tlearn: 71356.3890504\ttest: 74007.8019133\tbest: 73971.4858372 (89)\ttotal: 2.98s\tremaining: 6.64s\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10063)\u001b[0m bestTest = 79735.72845\n",
      "\u001b[36m(_ray_fit pid=10063)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m bestTest = 73971.48584\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m bestIteration = 89\n",
      "\u001b[36m(_ray_fit pid=10023)\u001b[0m Shrink model to first 90 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10063)\u001b[0m 140:\tlearn: 71223.5987878\ttest: 74807.3538412\tbest: 74807.2815747 (131)\ttotal: 3.67s\tremaining: 6.37s\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m bestTest = 80187.59653\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=10063)\u001b[0m Shrink model to first 161 iterations.\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m 60:\tlearn: 71764.4399511\ttest: 76209.1044469\tbest: 76209.1044469 (60)\ttotal: 1.35s\tremaining: 7.32s\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m bestTest = 76205.45766\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m bestIteration = 62\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10114)\u001b[0m Shrink model to first 63 iterations.\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m 40:\tlearn: 71230.9910856\ttest: 82341.0320767\tbest: 82341.0320767 (40)\ttotal: 817ms\tremaining: 8.63s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m bestTest = 86583.63203\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m bestIteration = 2\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m bestTest = 82077.68639\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m bestIteration = 166\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m Shrink model to first 167 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-72963.065\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t52.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.73s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t360.8\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1386.0s of the 1385.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting ExtraTreesMSE_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t45.82s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74377.6985\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t338.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t12.58s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t355.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1034.07s of the 1033.8s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetFastAI_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.76%)\n",
      "\u001b[36m(_ray_fit pid=10323)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10323)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=10401)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10401)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10429)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=10429)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10429)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=10481)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10481)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=10481)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=10521)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=10521)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10521)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=10561)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10561)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=10561)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=10601)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=10601)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73349.2535\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t768.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t3.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t343.6\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 262.26s of the 261.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting XGBoost_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.51%)\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:43] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m   warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m [0]\tvalidation_0-rmse:77408.16935\n",
      "\u001b[36m(_ray_fit pid=10159)\u001b[0m 200:\tlearn: 69532.0098187\ttest: 82084.0482313\tbest: 82077.6863877 (166)\ttotal: 4.21s\tremaining: 5.71s\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10739)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10739)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10739)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10739)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10739)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:51] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10738)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m [0]\tvalidation_0-rmse:66127.62197\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:58] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10799)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m [0]\tvalidation_0-rmse:80105.80584\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10861)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:06] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:56:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10863)\u001b[0m This warning will only be shown once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m [0]\tvalidation_0-rmse:80501.17583\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10924)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73495.7824\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t27.81s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.7s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t354.9\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 231.31s of the 231.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting NeuralNetTorch_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.98%)\n",
      "\u001b[36m(_ray_fit pid=11083)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 13)\n",
      "\u001b[36m(_ray_fit pid=11083)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=11083)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:06] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=10926)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=11180)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 13)\n",
      "\u001b[36m(_ray_fit pid=11180)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11180)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11152)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\u001b[36m(_ray_fit pid=11224)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11224)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11224)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 13)\n",
      "\u001b[36m(_ray_fit pid=11222)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\u001b[36m(_ray_fit pid=11294)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11294)\u001b[0m   self.model = torch.load(net_filename)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11294)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 13)\n",
      "\u001b[36m(_ray_fit pid=11322)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-73396.3236\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t200.37s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t2.69s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t349.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 27.86s of the 27.59s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting LightGBMLarge_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "\u001b[36m(_ray_fit pid=11322)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=11322)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.33%)\n",
      "\u001b[36m(_ray_fit pid=11462)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11462)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11462)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11462)\u001b[0m \tRan out of time, early stopping on iteration 25. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11462)\u001b[0m \t[25]\tvalid_set's rmse: 65346.9\n",
      "\u001b[36m(_ray_fit pid=11461)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11518)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11461)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11461)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11461)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11461)\u001b[0m \t[26]\tvalid_set's rmse: 73823.9\n",
      "\u001b[36m(_ray_fit pid=11518)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11518)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11518)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11518)\u001b[0m \t[26]\tvalid_set's rmse: 61516.7\n",
      "\u001b[36m(_ray_fit pid=11520)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11578)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11520)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11520)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11520)\u001b[0m \tRan out of time, early stopping on iteration 25. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11520)\u001b[0m \t[25]\tvalid_set's rmse: 80318.4\n",
      "\u001b[36m(_ray_fit pid=11578)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11578)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11578)\u001b[0m \tRan out of time, early stopping on iteration 25. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11578)\u001b[0m \t[25]\tvalid_set's rmse: 75396.2\n",
      "\u001b[36m(_ray_fit pid=11580)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11636)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11580)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11580)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11580)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11580)\u001b[0m \t[26]\tvalid_set's rmse: 76674.6\n",
      "\u001b[36m(_ray_fit pid=11636)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11636)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=11636)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11636)\u001b[0m \t[26]\tvalid_set's rmse: 77280.5\n",
      "\u001b[36m(_ray_fit pid=11638)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-74591.9704\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t40.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.69s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t361.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r177_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r79_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r131_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r191_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r9_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r96_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r22_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r33_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r42_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r137_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r102_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r13_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r195_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r188_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r145_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r89_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r30_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r130_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r86_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r50_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r11_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r194_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r172_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r69_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r103_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r14_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r161_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r143_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r70_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r156_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r196_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r39_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r167_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r95_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r41_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r98_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r15_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r158_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r86_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r37_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r197_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r49_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r49_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r143_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r127_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r134_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r34_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r94_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r143_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r128_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r111_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r31_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r4_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r65_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r88_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r30_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r49_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r5_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r87_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r71_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r143_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r178_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r166_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r31_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r185_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r160_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r60_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_ray_fit pid=11638)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=11638)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r15_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r135_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r22_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r69_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r6_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r138_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r121_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r172_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r180_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r76_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r197_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r121_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r127_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping RandomForest_r16_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r194_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r12_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r135_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r4_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping ExtraTrees_r126_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r36_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r100_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r163_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping CatBoost_r198_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetFastAI_r187_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r19_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r95_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping XGBoost_r34_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping LightGBM_r42_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r1_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Skipping NeuralNetTorch_r89_BAG_L2 due to lack of time remaining.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Model configs that will be trained (in order):\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -17.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tFitting WeightedEnsemble_L3 with 'num_gpus': 0, 'num_cpus': 4\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Ensemble size: 24\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Ensemble weights: \n",
      "\u001b[36m(_dystack pid=167)\u001b[0m [0.         0.         0.08333333 0.         0.         0.04166667\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m  0.04166667 0.         0.         0.         0.         0.04166667\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m  0.04166667 0.         0.         0.08333333 0.         0.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m  0.125      0.04166667 0.41666667 0.04166667 0.04166667 0.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m  0.        ]\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t1.76s\t= Estimated out-of-fold prediction time...\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L3/utils/oof.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L3/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L2': 0.417, 'LightGBMXT_BAG_L2': 0.125, 'NeuralNetFastAI_BAG_L1': 0.083, 'NeuralNetFastAI_r145_BAG_L1': 0.083, 'CatBoost_r177_BAG_L1': 0.042, 'NeuralNetTorch_r79_BAG_L1': 0.042, 'ExtraTrees_r42_BAG_L1': 0.042, 'NeuralNetFastAI_r102_BAG_L1': 0.042, 'RandomForestMSE_BAG_L2': 0.042, 'ExtraTreesMSE_BAG_L2': 0.042, 'NeuralNetFastAI_BAG_L2': 0.042}\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t-72915.3716\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m \t318.0\t = Inference  throughput (rows/s | 20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m AutoGluon training complete, total runtime = 9913.18s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 318.0 rows/s (20948 batch size)\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/trainer.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/learner.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/predictor.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/version.txt with contents \"1.1.1\"\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Saving AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/metadata.json\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "\u001b[36m(_ray_fit pid=11638)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=11638)\u001b[0m \t[26]\tvalid_set's rmse: 83787.3\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r177_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r131_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r191_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r9_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r96_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r33_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTrees_r42_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r137_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r102_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_r13_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForest_r195_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r188_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_r145_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_r89_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_r130_BAG_L1/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMXT_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBM_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/CatBoost_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/XGBoost_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Loading: AutogluonModels/ag-20240910_171549/ds_sub_fit/sub_fit_ho/models/WeightedEnsemble_L3/model.pkl\n",
      "\u001b[36m(_dystack pid=167)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0               CatBoost_BAG_L2  -69527.444411 -72963.064973  root_mean_squared_error       53.627936     100.162531  6524.698994                 0.310443                0.725462          52.041706            2       True         32\n",
      "1           WeightedEnsemble_L3  -69546.326212 -72915.371564  root_mean_squared_error       61.573214     131.885573  9396.910496                 0.007883                0.002812           0.642004            3       True         38\n",
      "2        NeuralNetFastAI_BAG_L2  -69590.328772 -73349.253521  root_mean_squared_error       57.421658     103.061082  7241.267563                 4.104165                3.624013         768.610274            2       True         34\n",
      "3             LightGBMXT_BAG_L2  -69635.902568 -73075.895789  root_mean_squared_error       53.914663     100.200438  6525.348714                 0.597171                0.763369          52.691426            2       True         29\n",
      "4           WeightedEnsemble_L2  -69642.832150 -72969.672930  root_mean_squared_error       33.381500      54.087467  4349.543393                 0.008628                0.003553           0.656269            2       True         28\n",
      "5                XGBoost_BAG_L2  -69658.703143 -73495.782382  root_mean_squared_error       54.876497     101.133748  6500.467522                 1.559004                1.696680          27.810233            2       True         35\n",
      "6   NeuralNetFastAI_r145_BAG_L1  -69774.317234 -73488.858545  root_mean_squared_error        7.688439       6.824673   912.322681                 7.688439                6.824673         912.322681            1       True         24\n",
      "7   NeuralNetFastAI_r191_BAG_L1  -69802.238841 -73543.978455  root_mean_squared_error        5.547595       3.869723   941.600596                 5.547595                3.869723         941.600596            1       True         13\n",
      "8            CatBoost_r9_BAG_L1  -69802.397465 -73237.922048  root_mean_squared_error        0.564665       1.015070    39.161420                 0.564665                1.015070          39.161420            1       True         14\n",
      "9               LightGBM_BAG_L2  -69811.439744 -73536.590988  root_mean_squared_error       53.854090     100.070759  6522.337775                 0.536597                0.633691          49.680487            2       True         30\n",
      "10         LightGBM_r188_BAG_L1  -69824.810295 -73463.138785  root_mean_squared_error        1.314080       2.390513    49.776393                 1.314080                2.390513          49.776393            1       True         23\n",
      "11          LightGBM_r96_BAG_L1  -69835.094624 -73278.160433  root_mean_squared_error        2.825073       5.214593    85.120601                 2.825073                5.214593          85.120601            1       True         15\n",
      "12            LightGBMXT_BAG_L1  -69840.008117 -73332.525824  root_mean_squared_error        0.638918       1.306026    40.709116                 0.638918                1.306026          40.709116            1       True          1\n",
      "13       NeuralNetFastAI_BAG_L1  -69840.658725 -73559.985460  root_mean_squared_error        6.833850       3.745149   796.941826                 6.833850                3.745149         796.941826            1       True          6\n",
      "14          CatBoost_r13_BAG_L1  -69879.355329 -73214.967313  root_mean_squared_error        1.123999       2.158876   109.562546                 1.123999                2.158876         109.562546            1       True         21\n",
      "15  NeuralNetFastAI_r102_BAG_L1  -69879.812820 -73715.457467  root_mean_squared_error        1.196300       0.956330   150.523271                 1.196300                0.956330         150.523271            1       True         20\n",
      "16         LightGBM_r130_BAG_L1  -69886.365677 -73485.826884  root_mean_squared_error        0.751031       1.184217    40.050527                 0.751031                1.184217          40.050527            1       True         27\n",
      "17              CatBoost_BAG_L1  -69905.480114 -73252.385066  root_mean_squared_error        0.972434       1.113823    64.573723                 0.972434                1.113823          64.573723            1       True          4\n",
      "18         CatBoost_r177_BAG_L1  -69907.600097 -73245.105286  root_mean_squared_error        0.453287       0.980669    49.376591                 0.453287                0.980669          49.376591            1       True         10\n",
      "19        NeuralNetTorch_BAG_L2  -69934.512636 -73396.323582  root_mean_squared_error       56.311840     102.123442  6673.032033                 2.994347                2.686373         200.374745            2       True         36\n",
      "20         LightGBM_r131_BAG_L1  -69982.881513 -73543.301162  root_mean_squared_error        1.731987       3.227178    72.751308                 1.731987                3.227178          72.751308            1       True         12\n",
      "21         CatBoost_r137_BAG_L1  -70002.401852 -73275.402400  root_mean_squared_error        0.454561       1.014483    53.135396                 0.454561                1.014483          53.135396            1       True         19\n",
      "22              LightGBM_BAG_L1  -70088.692617 -73655.391186  root_mean_squared_error        0.474872       0.774039    37.608950                 0.474872                0.774039          37.608950            1       True          2\n",
      "23       RandomForestMSE_BAG_L2  -70148.412910 -74823.617532  root_mean_squared_error       55.055402     114.188547  8184.886950                 1.737910               14.751478        1712.229661            2       True         31\n",
      "24           XGBoost_r89_BAG_L1  -70314.629199 -73667.086466  root_mean_squared_error        1.413751       1.065375    21.721183                 1.413751                1.065375          21.721183            1       True         25\n",
      "25         ExtraTreesMSE_BAG_L2  -70317.841382 -74377.698507  root_mean_squared_error       54.815643     112.018438  6810.695425                 1.498150               12.581370         338.038136            2       True         33\n",
      "26         LightGBMLarge_BAG_L1  -70320.561489 -74049.377654  root_mean_squared_error        0.753090       1.284085    47.407495                 0.753090                1.284085          47.407495            1       True          9\n",
      "27        NeuralNetTorch_BAG_L1  -70491.746385 -73822.790224  root_mean_squared_error        1.655257       2.665208   708.334330                 1.655257                2.665208         708.334330            1       True          8\n",
      "28    NeuralNetTorch_r79_BAG_L1  -70550.447741 -73874.985003  root_mean_squared_error        1.889818       2.701895   706.606216                 1.889818                2.701895         706.606216            1       True         11\n",
      "29           XGBoost_r33_BAG_L1  -70749.318578 -74174.911154  root_mean_squared_error        2.045265       1.259775    31.187686                 2.045265                1.259775          31.187686            1       True         17\n",
      "30               XGBoost_BAG_L1  -70757.512148 -74076.048491  root_mean_squared_error        1.395818       1.128880    22.539218                 1.395818                1.128880          22.539218            1       True          7\n",
      "31         LightGBMLarge_BAG_L2  -71219.830891 -74591.970407  root_mean_squared_error       53.714936     100.122345  6513.541139                 0.397444                0.685277          40.883851            2       True         37\n",
      "32     RandomForest_r195_BAG_L1  -72601.673782 -76625.830957  root_mean_squared_error        3.189030      14.231178   310.845310                 3.189030               14.231178         310.845310            1       True         22\n",
      "33        ExtraTrees_r42_BAG_L1  -72614.482005 -76224.693209  root_mean_squared_error        1.659462      11.128190   160.667030                 1.659462               11.128190         160.667030            1       True         18\n",
      "34         ExtraTreesMSE_BAG_L1  -73074.664964 -76550.353155  root_mean_squared_error        1.309786      11.201540   206.775541                 1.309786               11.201540         206.775541            1       True          5\n",
      "35    NeuralNetTorch_r30_BAG_L1  -73102.666618 -78670.444599  root_mean_squared_error        2.049723       2.965934   190.622732                 2.049723                2.965934         190.622732            1       True         26\n",
      "36       RandomForestMSE_BAG_L1  -73434.827920 -77530.819187  root_mean_squared_error        1.404501      11.554843   328.161303                 1.404501               11.554843         328.161303            1       True          3\n",
      "37    NeuralNetTorch_r22_BAG_L1  -87811.346962 -90504.640820  root_mean_squared_error        1.980903       2.474803   294.574301                 1.980903                2.474803         294.574301            1       True         16\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t9991s\t = DyStack   runtime |\t29609s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Saving AutogluonModels/ag-20240910_171549/learner.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 29609s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240910_171549\"\n",
      "Train Data Rows:    188533\n",
      "Train Data Columns: 12\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29129.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 116.72 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('int64', 'int')     : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('object', 'object') : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['bool'])    : 1 | ['clean_title']\n",
      "\t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])       : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['bool']) : 1 | ['clean_title']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', [])       : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['bool']) : 1 | ['clean_title']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t4 features in original data used to generate 4 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', [])       : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                   : 7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('category', ['text_as_category']) : 1 | ['engine']\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('float', ['text_special']) : 7 | ['engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', 'engine.special_ratio', 'engine.symbol_ratio..', ...]\n",
      "\t\t\t\t\t('int', ['text_special'])   : 5 | ['engine.char_count', 'engine.word_count', 'engine.symbol_count..', 'engine.symbol_count. ', 'engine.symbol_count./']\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 12 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t0.4s = Fit runtime\n",
      "\t\t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\t\t1 duplicate columns removed: ['engine.symbol_count. ']\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t0.5s = Fit runtime\n",
      "\t\t\t\t11 features in original data used to generate 11 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t6.3s = Fit runtime\n",
      "\t\t\t1 features in original data used to generate 11 features in processed data.\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['engine']\n",
      "\t\t\t\tCountVectorizer(dtype=<class 'numpy.uint8'>, max_features=10000, min_df=30,\n",
      "                ngram_range=(1, 3))\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 116\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['text']) : 1 | ['engine']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['text_ngram']) : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t\t5.7s = Fit runtime\n",
      "\t\t\t1 features in original data used to generate 117 features in processed data.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :   7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :   1 | ['engine']\n",
      "\t\t\t\t('int', [])                         :   3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) :  11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :   1 | ['clean_title']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :   7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :   1 | ['engine']\n",
      "\t\t\t\t('int', [])                         :   3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) :  11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :   1 | ['clean_title']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 117 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t\t0.7s = Fit runtime\n",
      "\t\t\t140 features in original data used to generate 140 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\t52 duplicate columns removed: ['__nlp__.twin turbo', '__nlp__.0l cylinder engine', '__nlp__.0l straight', '__nlp__.0l straight cylinder', '__nlp__.7l cylinder engine', '__nlp__.0l v6 cylinder', '__nlp__.2l cylinder', '__nlp__.2l cylinder engine', '__nlp__.3l cylinder', '__nlp__.3l cylinder engine', '__nlp__.4l cylinder', '__nlp__.4l cylinder engine', '__nlp__.5l cylinder', '__nlp__.5l cylinder engine', '__nlp__.5l v6 cylinder', '__nlp__.6l cylinder', '__nlp__.6l cylinder engine', '__nlp__.6l v6 cylinder', '__nlp__.flat cylinder', '__nlp__.flat cylinder engine', '__nlp__.8l cylinder', '__nlp__.8l cylinder engine', '__nlp__.0hp electric motor', '__nlp__.cylinder engine flex', '__nlp__.electric fuel', '__nlp__.electric fuel system', '__nlp__.electric motor', '__nlp__.electric motor electric', '__nlp__.engine flex', '__nlp__.engine flex fuel', '__nlp__.flex', '__nlp__.flex fuel', '__nlp__.flex fuel capability', '__nlp__.fuel capability', '__nlp__.fuel system', '__nlp__.motor', '__nlp__.motor electric', '__nlp__.motor electric fuel', '__nlp__.system', '__nlp__.diesel fuel', '__nlp__.engine diesel', '__nlp__.engine diesel fuel', '__nlp__.engine gas', '__nlp__.engine gas electric', '__nlp__.gas electric', '__nlp__.gas electric hybrid', '__nlp__.gasoline', '__nlp__.gasoline fuel', '__nlp__.i4 16v', '__nlp__.straight cylinder', '__nlp__.straight cylinder engine', '__nlp__.v6 cylinder engine']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\t\t\t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\t\t\t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t\t6.6s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('int64', 'int')     : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t('object', 'object') : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 3 | ['id', 'model_year', 'milage']\n",
      "\t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t('object', ['text']) : 1 | ['engine']\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :  8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('int64', 'int')         :  3 | ['id', 'model_year', 'milage']\n",
      "\t\t('int8', 'int')          :  1 | ['clean_title']\n",
      "\t\t('uint16', 'int')        : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t\t('uint8', 'int')         : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\t\t('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\t\t('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t20.9s = Fit runtime\n",
      "\t12 features in original data used to generate 88 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 21.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutogluonModels/ag-20240910_171549/learner.pkl\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}, {'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17}}, {'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19}}, {'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26}}, {'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35}}, {'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38}}, {'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41}}, {'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49}}, {'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52}}, {'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59}}, {'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60}}, {'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65}}, {'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77}}, {'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79}}, {'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84}}, {'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87}}, {'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92}}, {'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96}}, {'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge', {'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3}}, {'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6}}, {'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14}}, {'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18}}, {'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27}}, {'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31}}, {'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37}}, {'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44}}, {'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48}}, {'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56}}, {'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69}}, {'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74}}, {'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10}}, {'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12}}, {'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24}}, {'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33}}, {'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50}}, {'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67}}, {'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72}}, {'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76}}, {'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83}}, {'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89}}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}, {'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'name_suffix': '_r194', 'priority': -22}}, {'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'name_suffix': '_r98', 'priority': -36}}, {'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'name_suffix': '_r49', 'priority': -57}}, {'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'name_suffix': '_r31', 'priority': -64}}, {'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'name_suffix': '_r22', 'priority': -70}}, {'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'name_suffix': '_r95', 'priority': -93}}, {'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'name_suffix': '_r34', 'priority': -94}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}, {'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15}}, {'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21}}, {'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25}}, {'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28}}, {'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30}}, {'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34}}, {'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40}}, {'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46}}, {'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51}}, {'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54}}, {'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55}}, {'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66}}, {'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71}}, {'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73}}, {'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75}}, {'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80}}, {'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82}}, {'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85}}, {'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88}}, {'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13}}, {'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32}}, {'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45}}, {'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47}}, {'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63}}, {'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68}}, {'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9}}, {'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23}}, {'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43}}, {'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53}}, {'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62}}, {'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78}}, {'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Saving AutogluonModels/ag-20240910_171549/utils/data/X.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/utils/data/y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r177_BAG_L1: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r79_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r131_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r191_BAG_L1: \t{'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r9_BAG_L1: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r96_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r22_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r33_BAG_L1: \t{'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r33', 'priority': -8, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r42_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r137_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r102_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r13_BAG_L1: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r195_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r188_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r145_BAG_L1: \t{'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r89_BAG_L1: \t{'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r89', 'priority': -16, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r30_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r130_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r86_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r50_BAG_L1: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r11_BAG_L1: \t{'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r194_BAG_L1: \t{'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r194', 'priority': -22, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r172_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r69_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r103_BAG_L1: \t{'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r14_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r161_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r143_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r70_BAG_L1: \t{'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r156_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r196_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r39_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r167_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r95_BAG_L1: \t{'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r41_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r98_BAG_L1: \t{'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r98', 'priority': -36, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r15_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r158_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r86_BAG_L1: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r37_BAG_L1: \t{'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r197_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r49_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r49_BAG_L1: \t{'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r143_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r127_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r134_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r34_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r94_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r143_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r128_BAG_L1: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r111_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r31_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r4_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r65_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r88_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r30_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r49_BAG_L1: \t{'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r49', 'priority': -57, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r5_BAG_L1: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r87_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r71_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r143_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r178_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForest_r166_BAG_L1: \t{'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tXGBoost_r31_BAG_L1: \t{'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r31', 'priority': -64, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r185_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r160_BAG_L1: \t{'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r60_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r15_BAG_L1: \t{'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r135_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r22_BAG_L1: \t{'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r22', 'priority': -70, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r69_BAG_L1: \t{'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r6_BAG_L1: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r138_BAG_L1: \t{'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r121_BAG_L1: \t{'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r172_BAG_L1: \t{'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r180_BAG_L1: \t{'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r76_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r197_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetTorch_r121_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r127_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r16_BAG_L1: \t{'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r194_BAG_L1: \t{'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r12_BAG_L1: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r135_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r4_BAG_L1: \t{'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r126_BAG_L1: \t{'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetTorch_r36_BAG_L1: \t{'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r100_BAG_L1: \t{'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r163_BAG_L1: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r198_BAG_L1: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r187_BAG_L1: \t{'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r19_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r95_BAG_L1: \t{'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r95', 'priority': -93, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r34_BAG_L1: \t{'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r34', 'priority': -94, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r42_BAG_L1: \t{'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r1_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r89_BAG_L1: \t{'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 19720.46s of the 29587.96s of remaining time.\n",
      "\tFitting LightGBMXT_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.72%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/model.pkl\n",
      "\t-72930.0936\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.44s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "\t15334.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 19670.94s of the 29538.44s of remaining time.\n",
      "\tFitting LightGBM_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/model.pkl\n",
      "\t-73184.4293\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.98s\t = Training   runtime\n",
      "\t1.03s\t = Validation runtime\n",
      "\t22925.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 19626.38s of the 29493.88s of remaining time.\n",
      "\tFitting RandomForestMSE_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "\t38.56s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "\t-76887.7721\t = Validation score   (-root_mean_squared_error)\n",
      "\t360.31s\t = Training   runtime\n",
      "\t12.45s\t = Validation runtime\n",
      "\t15142.9\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 19252.37s of the 29119.87s of remaining time.\n",
      "\tFitting CatBoost_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.78%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/model.pkl\n",
      "\t-72856.0806\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.37s\t = Training   runtime\n",
      "\t1.26s\t = Validation runtime\n",
      "\t18662.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 19176.37s of the 29043.86s of remaining time.\n",
      "\tFitting ExtraTreesMSE_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "\t38.46s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "\t-76043.5333\t = Validation score   (-root_mean_squared_error)\n",
      "\t249.44s\t = Training   runtime\n",
      "\t12.4s\t = Validation runtime\n",
      "\t15209.7\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 18913.2s of the 28780.7s of remaining time.\n",
      "\tFitting NeuralNetFastAI_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.19%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "\t-73311.2132\t = Validation score   (-root_mean_squared_error)\n",
      "\t901.73s\t = Training   runtime\n",
      "\t4.03s\t = Validation runtime\n",
      "\t5845.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 18008.53s of the 27876.03s of remaining time.\n",
      "\tFitting XGBoost_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.04%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/model.pkl\n",
      "\t-73688.9903\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.08s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "\t17657.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 17982.67s of the 27850.16s of remaining time.\n",
      "\tFitting NeuralNetTorch_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "\t-73476.8817\t = Validation score   (-root_mean_squared_error)\n",
      "\t765.56s\t = Training   runtime\n",
      "\t1.4s\t = Validation runtime\n",
      "\t16871.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 17214.39s of the 27081.89s of remaining time.\n",
      "\tFitting LightGBMLarge_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.93%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "\t-73561.2232\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.3s\t = Training   runtime\n",
      "\t1.44s\t = Validation runtime\n",
      "\t16328.0\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 17161.16s of the 27028.65s of remaining time.\n",
      "\tFitting CatBoost_r177_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.80%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/model.pkl\n",
      "\t-72830.9904\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.66s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "\t20076.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 17100.84s of the 26968.34s of remaining time.\n",
      "\tFitting NeuralNetTorch_r79_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/model.pkl\n",
      "\t-73470.6762\t = Validation score   (-root_mean_squared_error)\n",
      "\t654.09s\t = Training   runtime\n",
      "\t1.53s\t = Validation runtime\n",
      "\t15358.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 16444.18s of the 26311.67s of remaining time.\n",
      "\tFitting LightGBM_r131_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.79%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/model.pkl\n",
      "\t-73091.8376\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.87s\t = Training   runtime\n",
      "\t4.01s\t = Validation runtime\n",
      "\t5878.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 16354.29s of the 26221.79s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r191_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.20%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/model.pkl\n",
      "\t-73200.0912\t = Validation score   (-root_mean_squared_error)\n",
      "\t940.77s\t = Training   runtime\n",
      "\t3.95s\t = Validation runtime\n",
      "\t5972.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 15410.43s of the 25277.93s of remaining time.\n",
      "\tFitting CatBoost_r9_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.15%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/model.pkl\n",
      "\t-72827.1568\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.25s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "\t20905.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 15368.69s of the 25236.18s of remaining time.\n",
      "\tFitting LightGBM_r96_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.71%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/model.pkl\n",
      "\t-72879.2757\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.43s\t = Training   runtime\n",
      "\t7.07s\t = Validation runtime\n",
      "\t3334.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 15263.04s of the 25130.54s of remaining time.\n",
      "\tFitting NeuralNetTorch_r22_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/model.pkl\n",
      "\t-90209.5351\t = Validation score   (-root_mean_squared_error)\n",
      "\t307.07s\t = Training   runtime\n",
      "\t1.3s\t = Validation runtime\n",
      "\t18139.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 14953.32s of the 24820.82s of remaining time.\n",
      "\tFitting XGBoost_r33_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.83%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/model.pkl\n",
      "\t-73599.1879\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.52s\t = Training   runtime\n",
      "\t1.37s\t = Validation runtime\n",
      "\t17147.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 14916.21s of the 24783.71s of remaining time.\n",
      "\tFitting ExtraTrees_r42_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/model_template.pkl\n",
      "\t35.02s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/model.pkl\n",
      "\t-75851.3192\t = Validation score   (-root_mean_squared_error)\n",
      "\t177.29s\t = Training   runtime\n",
      "\t12.07s\t = Validation runtime\n",
      "\t15623.4\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 14725.42s of the 24592.91s of remaining time.\n",
      "\tFitting CatBoost_r137_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.71%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/model.pkl\n",
      "\t-72882.8849\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.4s\t = Training   runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "\t24876.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 14663.59s of the 24531.09s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r102_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/model.pkl\n",
      "\t-73333.2282\t = Validation score   (-root_mean_squared_error)\n",
      "\t164.35s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "\t24533.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 14496.84s of the 24364.34s of remaining time.\n",
      "\tFitting CatBoost_r13_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.15%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/model.pkl\n",
      "\t-72780.5862\t = Validation score   (-root_mean_squared_error)\n",
      "\t167.45s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "\t7945.0\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 14326.54s of the 24194.04s of remaining time.\n",
      "\tFitting RandomForest_r195_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/model_template.pkl\n",
      "\t42.97s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/model.pkl\n",
      "\t-76049.9394\t = Validation score   (-root_mean_squared_error)\n",
      "\t341.46s\t = Training   runtime\n",
      "\t15.66s\t = Validation runtime\n",
      "\t12041.0\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 13966.65s of the 23834.15s of remaining time.\n",
      "\tFitting LightGBM_r188_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.90%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/model.pkl\n",
      "\t-72977.8751\t = Validation score   (-root_mean_squared_error)\n",
      "\t53.5s\t = Training   runtime\n",
      "\t2.56s\t = Validation runtime\n",
      "\t9210.0\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 13910.5s of the 23777.99s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r145_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/model.pkl\n",
      "\t-73170.293\t = Validation score   (-root_mean_squared_error)\n",
      "\t1781.51s\t = Training   runtime\n",
      "\t7.21s\t = Validation runtime\n",
      "\t3267.6\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 12125.4s of the 21992.9s of remaining time.\n",
      "\tFitting XGBoost_r89_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.04%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/model.pkl\n",
      "\t-73265.3685\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.93s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "\t18419.0\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 12098.68s of the 21966.18s of remaining time.\n",
      "\tFitting NeuralNetTorch_r30_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/model.pkl\n",
      "\t-82470.6763\t = Validation score   (-root_mean_squared_error)\n",
      "\t559.55s\t = Training   runtime\n",
      "\t1.63s\t = Validation runtime\n",
      "\t14429.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 11536.64s of the 21404.14s of remaining time.\n",
      "\tFitting LightGBM_r130_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.86%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/model.pkl\n",
      "\t-73023.7131\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.24s\t = Training   runtime\n",
      "\t1.66s\t = Validation runtime\n",
      "\t14175.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 11484.47s of the 21351.97s of remaining time.\n",
      "\tFitting NeuralNetTorch_r86_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/model.pkl\n",
      "\t-90209.5585\t = Validation score   (-root_mean_squared_error)\n",
      "\t326.63s\t = Training   runtime\n",
      "\t1.47s\t = Validation runtime\n",
      "\t16034.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 11155.1s of the 21022.6s of remaining time.\n",
      "\tFitting CatBoost_r50_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/model.pkl\n",
      "\t-72851.822\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.71s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "\t20995.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 11120.68s of the 20988.18s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r11_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.22%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/model.pkl\n",
      "\t-73733.7146\t = Validation score   (-root_mean_squared_error)\n",
      "\t1740.05s\t = Training   runtime\n",
      "\t7.08s\t = Validation runtime\n",
      "\t3328.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 9376.96s of the 19244.45s of remaining time.\n",
      "\tFitting XGBoost_r194_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.15%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/model.pkl\n",
      "\t-73750.2081\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.08s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "\t23638.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 9348.04s of the 19215.54s of remaining time.\n",
      "\tFitting ExtraTrees_r172_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/model_template.pkl\n",
      "\t38.84s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/model.pkl\n",
      "\t-73176.8604\t = Validation score   (-root_mean_squared_error)\n",
      "\t259.41s\t = Training   runtime\n",
      "\t13.19s\t = Validation runtime\n",
      "\t14297.3\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 9074.33s of the 18941.83s of remaining time.\n",
      "\tFitting CatBoost_r69_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/model.pkl\n",
      "\t-72881.4448\t = Validation score   (-root_mean_squared_error)\n",
      "\t64.96s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "\t19639.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 9006.67s of the 18874.17s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r103_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/model.pkl\n",
      "\t-73319.0303\t = Validation score   (-root_mean_squared_error)\n",
      "\t980.93s\t = Training   runtime\n",
      "\t3.92s\t = Validation runtime\n",
      "\t6007.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 8022.45s of the 17889.94s of remaining time.\n",
      "\tFitting NeuralNetTorch_r14_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/model.pkl\n",
      "\t-73511.6079\t = Validation score   (-root_mean_squared_error)\n",
      "\t473.56s\t = Training   runtime\n",
      "\t1.35s\t = Validation runtime\n",
      "\t17449.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 7546.3s of the 17413.8s of remaining time.\n",
      "\tFitting LightGBM_r161_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.15%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/model.pkl\n",
      "\t-73118.8401\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.59s\t = Training   runtime\n",
      "\t8.65s\t = Validation runtime\n",
      "\t2725.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 7430.88s of the 17298.38s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r143_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/model.pkl\n",
      "\t-72883.0979\t = Validation score   (-root_mean_squared_error)\n",
      "\t404.45s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "\t17894.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 7023.83s of the 16891.33s of remaining time.\n",
      "\tFitting CatBoost_r70_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.81%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/model.pkl\n",
      "\t-72854.2991\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.6s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "\t26018.0\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 6987.56s of the 16855.06s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r156_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/model.pkl\n",
      "\t-73151.1855\t = Validation score   (-root_mean_squared_error)\n",
      "\t138.62s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "\t27496.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 6846.54s of the 16714.04s of remaining time.\n",
      "\tFitting LightGBM_r196_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.94%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/model.pkl\n",
      "\t-72932.6376\t = Validation score   (-root_mean_squared_error)\n",
      "\t259.46s\t = Training   runtime\n",
      "\t39.25s\t = Validation runtime\n",
      "\t600.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 6579.37s of the 16446.87s of remaining time.\n",
      "\tFitting RandomForest_r39_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/model_template.pkl\n",
      "\t43.08s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/model.pkl\n",
      "\t-74335.3716\t = Validation score   (-root_mean_squared_error)\n",
      "\t319.03s\t = Training   runtime\n",
      "\t14.96s\t = Validation runtime\n",
      "\t12604.4\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 6243.14s of the 16110.64s of remaining time.\n",
      "\tFitting CatBoost_r167_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.90%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/model.pkl\n",
      "\t-72823.818\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.15s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "\t23748.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 6188.5s of the 16056.0s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r95_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/model.pkl\n",
      "\t-73125.6464\t = Validation score   (-root_mean_squared_error)\n",
      "\t1884.52s\t = Training   runtime\n",
      "\t7.29s\t = Validation runtime\n",
      "\t3232.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 4300.5s of the 14168.0s of remaining time.\n",
      "\tFitting NeuralNetTorch_r41_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/model.pkl\n",
      "\t-73441.522\t = Validation score   (-root_mean_squared_error)\n",
      "\t744.57s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "\t15211.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 3553.34s of the 13420.83s of remaining time.\n",
      "\tFitting XGBoost_r98_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.38%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/model.pkl\n",
      "\t-73296.9599\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.78s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "\t19751.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 3469.04s of the 13336.53s of remaining time.\n",
      "\tFitting LightGBM_r15_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.74%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/model.pkl\n",
      "\t-73117.184\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.14s\t = Training   runtime\n",
      "\t2.33s\t = Validation runtime\n",
      "\t10117.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 3406.21s of the 13273.71s of remaining time.\n",
      "\tFitting NeuralNetTorch_r158_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/model.pkl\n",
      "\t-90209.4595\t = Validation score   (-root_mean_squared_error)\n",
      "\t444.7s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "\t15346.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 2959.04s of the 12826.54s of remaining time.\n",
      "\tFitting CatBoost_r86_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.16%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/model.pkl\n",
      "\t-72810.3809\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.47s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "\t14995.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 2867.85s of the 12735.35s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r37_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/model.pkl\n",
      "\t-73093.3393\t = Validation score   (-root_mean_squared_error)\n",
      "\t721.49s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "\t9929.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 2143.58s of the 12011.08s of remaining time.\n",
      "\tFitting NeuralNetTorch_r197_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/model.pkl\n",
      "\t-86365.2253\t = Validation score   (-root_mean_squared_error)\n",
      "\t245.96s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "\t18965.6\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 1895.15s of the 11762.65s of remaining time.\n",
      "\tFitting CatBoost_r49_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/model.pkl\n",
      "\t-72897.3105\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.66s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "\t48674.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 1857.92s of the 11725.42s of remaining time.\n",
      "\tFitting ExtraTrees_r49_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/model_template.pkl\n",
      "\t38.36s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/model.pkl\n",
      "\t-74363.7938\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.13s\t = Training   runtime\n",
      "\t12.42s\t = Validation runtime\n",
      "\t15180.9\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 1804.05s of the 11671.55s of remaining time.\n",
      "\tFitting LightGBM_r143_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.00%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/model.pkl\n",
      "\t-73311.9491\t = Validation score   (-root_mean_squared_error)\n",
      "\t93.07s\t = Training   runtime\n",
      "\t4.17s\t = Validation runtime\n",
      "\t5657.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 1707.12s of the 11574.62s of remaining time.\n",
      "\tFitting RandomForest_r127_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/model_template.pkl\n",
      "\t61.13s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/model.pkl\n",
      "\t-73531.7714\t = Validation score   (-root_mean_squared_error)\n",
      "\t359.54s\t = Training   runtime\n",
      "\t13.9s\t = Validation runtime\n",
      "\t13563.6\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 1332.33s of the 11199.83s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r134_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/model.pkl\n",
      "\t-73276.0074\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.6s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "\t23253.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 1181.17s of the 11048.67s of remaining time.\n",
      "\tFitting RandomForest_r34_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/model_template.pkl\n",
      "\t34.55s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/model.pkl\n",
      "\t-72725.4399\t = Validation score   (-root_mean_squared_error)\n",
      "\t183.23s\t = Training   runtime\n",
      "\t10.04s\t = Validation runtime\n",
      "\t18773.1\t = Inference  throughput (rows/s | 188533 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 987.54s of the 10855.04s of remaining time.\n",
      "\tFitting LightGBM_r94_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.72%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/model.pkl\n",
      "\t-72893.0685\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.87s\t = Training   runtime\n",
      "\t2.85s\t = Validation runtime\n",
      "\t8278.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 928.03s of the 10795.53s of remaining time.\n",
      "\tFitting NeuralNetTorch_r143_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.67%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/model.pkl\n",
      "\t-73413.9235\t = Validation score   (-root_mean_squared_error)\n",
      "\t699.16s\t = Training   runtime\n",
      "\t1.45s\t = Validation runtime\n",
      "\t16303.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 226.47s of the 10093.97s of remaining time.\n",
      "\tFitting CatBoost_r128_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.18%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/model.pkl\n",
      "\t-72955.2544\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.52s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "\t25755.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 193.39s of the 10060.89s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r111_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.25%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/model.pkl\n",
      "\t-72987.9266\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.42s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "\t25401.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 53.43s of the 9920.93s of remaining time.\n",
      "\tFitting NeuralNetTorch_r31_BAG_L1 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.69%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/model.pkl\n",
      "\t-90229.7852\t = Validation score   (-root_mean_squared_error)\n",
      "\t53.01s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "\t19244.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r4_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r65_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r88_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r30_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r49_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r5_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r87_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r71_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r143_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r178_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r166_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r31_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r185_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r160_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r60_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r15_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r135_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r22_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r69_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r6_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r138_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r121_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r172_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r180_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r76_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r197_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r121_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r127_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r16_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r194_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r12_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r135_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r4_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r126_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r36_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r100_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r163_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r198_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r187_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r19_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r95_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r34_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r42_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r1_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r89_BAG_L1 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1972.04s of the 9865.14s of remaining time.\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Ensemble size: 21\n",
      "Ensemble weights: \n",
      "[0.         0.         0.04761905 0.         0.         0.\n",
      " 0.04761905 0.         0.         0.0952381  0.04761905 0.14285714\n",
      " 0.04761905 0.04761905 0.04761905 0.04761905 0.         0.\n",
      " 0.         0.04761905 0.0952381  0.19047619 0.         0.\n",
      " 0.0952381 ]\n",
      "\t2.04s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'RandomForest_r34_BAG_L1': 0.19, 'NeuralNetFastAI_r143_BAG_L1': 0.143, 'ExtraTrees_r172_BAG_L1': 0.095, 'RandomForest_r127_BAG_L1': 0.095, 'NeuralNetFastAI_r111_BAG_L1': 0.095, 'CatBoost_r177_BAG_L1': 0.048, 'CatBoost_r13_BAG_L1': 0.048, 'NeuralNetTorch_r14_BAG_L1': 0.048, 'NeuralNetFastAI_r156_BAG_L1': 0.048, 'LightGBM_r196_BAG_L1': 0.048, 'CatBoost_r167_BAG_L1': 0.048, 'NeuralNetFastAI_r95_BAG_L1': 0.048, 'NeuralNetFastAI_r37_BAG_L1': 0.048}\n",
      "\t-72490.9865\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t373.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r177_BAG_L2: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r79_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r131_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.7023601671276614, 'learning_rate': 0.012144796373999013, 'min_data_in_leaf': 14, 'num_leaves': 53, 'ag_args': {'name_suffix': '_r131', 'priority': -3, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r191_BAG_L2: \t{'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r9_BAG_L2: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r96_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.5636931414546802, 'learning_rate': 0.01518660230385841, 'min_data_in_leaf': 48, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r96', 'priority': -6, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r22_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r33_BAG_L2: \t{'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r33', 'priority': -8, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r42_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 18392, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r42', 'priority': -9, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r137_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.559174625782161, 'learning_rate': 0.04939557741379516, 'max_ctr_complexity': 3, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r137', 'priority': -10, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r102_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r13_BAG_L2: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3274013177541373, 'learning_rate': 0.017301189655111057, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r13', 'priority': -12, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r195_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 37308, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r195', 'priority': -13, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r188_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.8282601210460099, 'learning_rate': 0.033929021353492905, 'min_data_in_leaf': 6, 'num_leaves': 127, 'ag_args': {'name_suffix': '_r188', 'priority': -14, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r145_BAG_L2: \t{'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': -15, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r89_BAG_L2: \t{'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r89', 'priority': -16, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r30_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.24622382571353768, 'hidden_size': 159, 'learning_rate': 0.008507536855608535, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.8201539594953562e-06, 'ag_args': {'name_suffix': '_r30', 'priority': -17, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r130_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.6245777099925497, 'learning_rate': 0.04711573688184715, 'min_data_in_leaf': 56, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r130', 'priority': -18, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r86_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.09976801642258049, 'hidden_size': 135, 'learning_rate': 0.001631450730978947, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 3.867683394425807e-05, 'ag_args': {'name_suffix': '_r86', 'priority': -19, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r50_BAG_L2: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7018061518087038, 'learning_rate': 0.07092851311746352, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r50', 'priority': -20, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r11_BAG_L2: \t{'bs': 128, 'emb_drop': 0.026897798530914306, 'epochs': 31, 'layers': [800, 400], 'lr': 0.08045277634470181, 'ps': 0.4569532219038436, 'ag_args': {'name_suffix': '_r11', 'priority': -21, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r194_BAG_L2: \t{'colsample_bytree': 0.9090166528779192, 'enable_categorical': True, 'learning_rate': 0.09290221350439203, 'max_depth': 7, 'min_child_weight': 0.8041986915994078, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r194', 'priority': -22, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r172_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 12845, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r172', 'priority': -23, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r69_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.0457098345001241, 'learning_rate': 0.050294288910022224, 'max_ctr_complexity': 5, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r69', 'priority': -24, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r103_BAG_L2: \t{'bs': 256, 'emb_drop': 0.1508701680951814, 'epochs': 46, 'layers': [400, 200], 'lr': 0.08794353125787312, 'ps': 0.19110623090573325, 'ag_args': {'name_suffix': '_r103', 'priority': -25, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r14_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.3905837860053583, 'hidden_size': 106, 'learning_rate': 0.0018297905295930797, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 9.178069874232892e-08, 'ag_args': {'name_suffix': '_r14', 'priority': -26, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r161_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.5898927512279213, 'learning_rate': 0.010464516487486093, 'min_data_in_leaf': 11, 'num_leaves': 252, 'ag_args': {'name_suffix': '_r161', 'priority': -27, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r143_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.6239200452002372, 'epochs': 39, 'layers': [200, 100, 50], 'lr': 0.07170321592506483, 'ps': 0.670815151683455, 'ag_args': {'name_suffix': '_r143', 'priority': -28, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r70_BAG_L2: \t{'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.3584121369544215, 'learning_rate': 0.03743901034980473, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r70', 'priority': -29, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r156_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.5055288166864152, 'epochs': 44, 'layers': [400], 'lr': 0.0047762208542912405, 'ps': 0.06572612802222005, 'ag_args': {'name_suffix': '_r156', 'priority': -30, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r196_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.5143401489640409, 'learning_rate': 0.00529479887023554, 'min_data_in_leaf': 6, 'num_leaves': 133, 'ag_args': {'name_suffix': '_r196', 'priority': -31, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r39_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 28310, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r39', 'priority': -32, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_r167_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.522712492188319, 'learning_rate': 0.08481607830570326, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r167', 'priority': -33, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r95_BAG_L2: \t{'bs': 128, 'emb_drop': 0.6656668277387758, 'epochs': 32, 'layers': [400, 200, 100], 'lr': 0.019326244622675428, 'ps': 0.04084945128641206, 'ag_args': {'name_suffix': '_r95', 'priority': -34, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r41_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.05488816803887784, 'hidden_size': 32, 'learning_rate': 0.0075612897834015985, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.652353009917866e-08, 'ag_args': {'name_suffix': '_r41', 'priority': -35, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r98_BAG_L2: \t{'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.007158072983547058, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r98', 'priority': -36, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r15_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.7421180622507277, 'learning_rate': 0.018603888565740096, 'min_data_in_leaf': 6, 'num_leaves': 22, 'ag_args': {'name_suffix': '_r15', 'priority': -37, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r158_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.01030258381183309, 'hidden_size': 111, 'learning_rate': 0.01845979186513771, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 0.00020238017476912164, 'ag_args': {'name_suffix': '_r158', 'priority': -38, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r86_BAG_L2: \t{'depth': 8, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6376578537958237, 'learning_rate': 0.032899230324940465, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r86', 'priority': -39, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r37_BAG_L2: \t{'bs': 512, 'emb_drop': 0.1567472816422661, 'epochs': 41, 'layers': [400, 200, 100], 'lr': 0.06831450078222204, 'ps': 0.4930900813464729, 'ag_args': {'name_suffix': '_r37', 'priority': -40, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r197_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.18109219857068798, 'hidden_size': 250, 'learning_rate': 0.00634181748507711, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 5.3861175580695396e-08, 'ag_args': {'name_suffix': '_r197', 'priority': -41, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r49_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.353268454214423, 'learning_rate': 0.06028218319511302, 'max_ctr_complexity': 1, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r49', 'priority': -42, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r49_BAG_L2: \t{'max_features': 'sqrt', 'max_leaf_nodes': 28532, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r49', 'priority': -43, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r143_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.9408897917880529, 'learning_rate': 0.01343464462043561, 'min_data_in_leaf': 21, 'num_leaves': 178, 'ag_args': {'name_suffix': '_r143', 'priority': -44, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r127_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 38572, 'min_samples_leaf': 5, 'ag_args': {'name_suffix': '_r127', 'priority': -45, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r134_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.006251885504130949, 'epochs': 47, 'layers': [800, 400], 'lr': 0.01329622020483052, 'ps': 0.2677080696008348, 'ag_args': {'name_suffix': '_r134', 'priority': -46, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r34_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 18242, 'min_samples_leaf': 40, 'ag_args': {'name_suffix': '_r34', 'priority': -47, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r94_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.4341088458599442, 'learning_rate': 0.04034449862560467, 'min_data_in_leaf': 33, 'num_leaves': 16, 'ag_args': {'name_suffix': '_r94', 'priority': -48, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r143_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.1703783780377607, 'hidden_size': 212, 'learning_rate': 0.0004107199833213839, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 1.105439140660822e-07, 'ag_args': {'name_suffix': '_r143', 'priority': -49, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r128_BAG_L2: \t{'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.640921865280573, 'learning_rate': 0.036232951900213306, 'max_ctr_complexity': 3, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r128', 'priority': -50, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r111_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.6343202884164582, 'epochs': 21, 'layers': [400, 200], 'lr': 0.08479209380262258, 'ps': 0.48362560779595565, 'ag_args': {'name_suffix': '_r111', 'priority': -51, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r31_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.013288954106470907, 'hidden_size': 81, 'learning_rate': 0.005340914647396154, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 8.762168370775353e-05, 'ag_args': {'name_suffix': '_r31', 'priority': -52, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r4_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 19935, 'min_samples_leaf': 20, 'ag_args': {'name_suffix': '_r4', 'priority': -53, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r65_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.22771721361129746, 'epochs': 38, 'layers': [400], 'lr': 0.0005383511954451698, 'ps': 0.3734259772256502, 'ag_args': {'name_suffix': '_r65', 'priority': -54, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r88_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.4329361816589235, 'epochs': 50, 'layers': [400], 'lr': 0.09501311551121323, 'ps': 0.2863378667611431, 'ag_args': {'name_suffix': '_r88', 'priority': -55, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r30_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.9773131270704629, 'learning_rate': 0.010534290864227067, 'min_data_in_leaf': 21, 'num_leaves': 111, 'ag_args': {'name_suffix': '_r30', 'priority': -56, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r49_BAG_L2: \t{'colsample_bytree': 0.7452294043087835, 'enable_categorical': False, 'learning_rate': 0.038404229910104046, 'max_depth': 7, 'min_child_weight': 0.5564183327139662, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r49', 'priority': -57, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r5_BAG_L2: \t{'depth': 4, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.894432181094842, 'learning_rate': 0.055078095725390575, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r5', 'priority': -58, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r87_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.36669080773207274, 'hidden_size': 95, 'learning_rate': 0.015280159186761077, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.3082489374636015e-08, 'ag_args': {'name_suffix': '_r87', 'priority': -59, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r71_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.3027114570947557, 'hidden_size': 196, 'learning_rate': 0.006482759295309238, 'num_layers': 1, 'use_batchnorm': False, 'weight_decay': 1.2806509958776e-12, 'ag_args': {'name_suffix': '_r71', 'priority': -60, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r143_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 1.6761016245166451, 'learning_rate': 0.06566144806528762, 'max_ctr_complexity': 2, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r143', 'priority': -61, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r178_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 29813, 'min_samples_leaf': 4, 'ag_args': {'name_suffix': '_r178', 'priority': -62, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForest_r166_BAG_L2: \t{'max_features': 'log2', 'max_leaf_nodes': 42644, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r166', 'priority': -63, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tXGBoost_r31_BAG_L2: \t{'colsample_bytree': 0.7506621909633511, 'enable_categorical': False, 'learning_rate': 0.009974712407899168, 'max_depth': 4, 'min_child_weight': 0.9238550485581797, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r31', 'priority': -64, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r185_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.12166942295569863, 'hidden_size': 151, 'learning_rate': 0.0018866871631794007, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 9.190843763153802e-05, 'ag_args': {'name_suffix': '_r185', 'priority': -65, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r160_BAG_L2: \t{'bs': 128, 'emb_drop': 0.3171659718142149, 'epochs': 20, 'layers': [400, 200, 100], 'lr': 0.03087210106068273, 'ps': 0.5909644730871169, 'ag_args': {'name_suffix': '_r160', 'priority': -66, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r60_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.3217885487525205, 'learning_rate': 0.05291587380674719, 'max_ctr_complexity': 5, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r60', 'priority': -67, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r15_BAG_L2: \t{'max_features': 0.75, 'max_leaf_nodes': 36230, 'min_samples_leaf': 3, 'ag_args': {'name_suffix': '_r15', 'priority': -68, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBM_r135_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.8254432681390782, 'learning_rate': 0.031251656439648626, 'min_data_in_leaf': 50, 'num_leaves': 210, 'ag_args': {'name_suffix': '_r135', 'priority': -69, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r22_BAG_L2: \t{'colsample_bytree': 0.6326947454697227, 'enable_categorical': False, 'learning_rate': 0.07792091886639502, 'max_depth': 6, 'min_child_weight': 1.0759464955561793, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r22', 'priority': -70, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r69_BAG_L2: \t{'bs': 128, 'emb_drop': 0.3209601865656554, 'epochs': 21, 'layers': [200, 100, 50], 'lr': 0.019935403046870463, 'ps': 0.19846319260751663, 'ag_args': {'name_suffix': '_r69', 'priority': -71, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r6_BAG_L2: \t{'depth': 4, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.5734131496361856, 'learning_rate': 0.08472519974533015, 'max_ctr_complexity': 3, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r6', 'priority': -72, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r138_BAG_L2: \t{'bs': 128, 'emb_drop': 0.08669109226243704, 'epochs': 45, 'layers': [800, 400], 'lr': 0.0041554361714983635, 'ps': 0.2669780074016213, 'ag_args': {'name_suffix': '_r138', 'priority': -73, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r121_BAG_L2: \t{'extra_trees': False, 'feature_fraction': 0.5730390983988963, 'learning_rate': 0.010305352949119608, 'min_data_in_leaf': 10, 'num_leaves': 215, 'ag_args': {'name_suffix': '_r121', 'priority': -74, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r172_BAG_L2: \t{'bs': 512, 'emb_drop': 0.05604276533830355, 'epochs': 32, 'layers': [400], 'lr': 0.027320709383189166, 'ps': 0.022591301744255762, 'ag_args': {'name_suffix': '_r172', 'priority': -75, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r180_BAG_L2: \t{'depth': 7, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 4.43335055453705, 'learning_rate': 0.055406199833457785, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r180', 'priority': -76, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r76_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.006531401073483156, 'hidden_size': 192, 'learning_rate': 0.012418052210914356, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 3.0406866089493607e-05, 'ag_args': {'name_suffix': '_r76', 'priority': -77, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r197_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 40459, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r197', 'priority': -78, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetTorch_r121_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.33926015213879396, 'hidden_size': 247, 'learning_rate': 0.0029983839090226075, 'num_layers': 5, 'use_batchnorm': False, 'weight_decay': 0.00038926240517691234, 'ag_args': {'name_suffix': '_r121', 'priority': -79, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r127_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.31956392388385874, 'epochs': 25, 'layers': [200, 100], 'lr': 0.08552736732040143, 'ps': 0.0934076022219228, 'ag_args': {'name_suffix': '_r127', 'priority': -80, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForest_r16_BAG_L2: \t{'max_features': 1.0, 'max_leaf_nodes': 48136, 'min_samples_leaf': 1, 'ag_args': {'name_suffix': '_r16', 'priority': -81, 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_r194_BAG_L2: \t{'bs': 256, 'emb_drop': 0.5117456464220826, 'epochs': 21, 'layers': [400, 200, 100], 'lr': 0.007212882302137526, 'ps': 0.2747013981281539, 'ag_args': {'name_suffix': '_r194', 'priority': -82, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r12_BAG_L2: \t{'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 4.835797074498082, 'learning_rate': 0.03534026385152556, 'max_ctr_complexity': 5, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r12', 'priority': -83, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r135_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.06134755114373829, 'hidden_size': 144, 'learning_rate': 0.005834535148903801, 'num_layers': 5, 'use_batchnorm': True, 'weight_decay': 2.0826540090463355e-09, 'ag_args': {'name_suffix': '_r135', 'priority': -84, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r4_BAG_L2: \t{'bs': 256, 'emb_drop': 0.06099050979107849, 'epochs': 39, 'layers': [200], 'lr': 0.04119582873110387, 'ps': 0.5447097256648953, 'ag_args': {'name_suffix': '_r4', 'priority': -85, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTrees_r126_BAG_L2: \t{'max_features': 'sqrt', 'max_leaf_nodes': 29702, 'min_samples_leaf': 2, 'ag_args': {'name_suffix': '_r126', 'priority': -86, 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetTorch_r36_BAG_L2: \t{'activation': 'elu', 'dropout_prob': 0.3457125770744979, 'hidden_size': 37, 'learning_rate': 0.006435774191713849, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 2.4012185204155345e-08, 'ag_args': {'name_suffix': '_r36', 'priority': -87, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r100_BAG_L2: \t{'bs': 2048, 'emb_drop': 0.6960805527533755, 'epochs': 38, 'layers': [800, 400], 'lr': 0.0007278526871749883, 'ps': 0.20495582200836318, 'ag_args': {'name_suffix': '_r100', 'priority': -88, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r163_BAG_L2: \t{'depth': 5, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.7454481983750014, 'learning_rate': 0.09328642499990342, 'max_ctr_complexity': 1, 'one_hot_max_size': 2, 'ag_args': {'name_suffix': '_r163', 'priority': -89, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tCatBoost_r198_BAG_L2: \t{'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 3.637071465711953, 'learning_rate': 0.04387418552563314, 'max_ctr_complexity': 4, 'one_hot_max_size': 5, 'ag_args': {'name_suffix': '_r198', 'priority': -90, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetFastAI_r187_BAG_L2: \t{'bs': 1024, 'emb_drop': 0.5074958658302495, 'epochs': 42, 'layers': [200, 100, 50], 'lr': 0.026342427824862867, 'ps': 0.34814978753283593, 'ag_args': {'name_suffix': '_r187', 'priority': -91, 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r19_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.2211285919550286, 'hidden_size': 196, 'learning_rate': 0.011307978270179143, 'num_layers': 1, 'use_batchnorm': True, 'weight_decay': 1.8441764217351068e-06, 'ag_args': {'name_suffix': '_r19', 'priority': -92, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r95_BAG_L2: \t{'colsample_bytree': 0.975937238416368, 'enable_categorical': False, 'learning_rate': 0.06634196266155237, 'max_depth': 5, 'min_child_weight': 1.4088437184127383, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r95', 'priority': -93, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_r34_BAG_L2: \t{'colsample_bytree': 0.546186944730449, 'enable_categorical': False, 'learning_rate': 0.029357102578825213, 'max_depth': 10, 'min_child_weight': 1.1532008198571337, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'name_suffix': '_r34', 'priority': -94, 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_r42_BAG_L2: \t{'extra_trees': True, 'feature_fraction': 0.4601361323873807, 'learning_rate': 0.07856777698860955, 'min_data_in_leaf': 12, 'num_leaves': 198, 'ag_args': {'name_suffix': '_r42', 'priority': -95, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r1_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.23713784729000734, 'hidden_size': 200, 'learning_rate': 0.00311256170909018, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 4.573016756474468e-08, 'ag_args': {'name_suffix': '_r1', 'priority': -96, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_r89_BAG_L2: \t{'activation': 'relu', 'dropout_prob': 0.33567564890346097, 'hidden_size': 245, 'learning_rate': 0.006746560197328548, 'num_layers': 3, 'use_batchnorm': True, 'weight_decay': 1.6470047305392933e-10, 'ag_args': {'name_suffix': '_r89', 'priority': -97, 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "Fitting 106 L2 models ...\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 9864.29s of the 9863.73s of remaining time.\n",
      "\tFitting LightGBMXT_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/model.pkl\n",
      "\t-72663.8513\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.9s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "\t138.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 9785.19s of the 9784.63s of remaining time.\n",
      "\tFitting LightGBM_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.73%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/model.pkl\n",
      "\t-73072.6289\t = Validation score   (-root_mean_squared_error)\n",
      "\t69.01s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "\t139.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 9712.9s of the 9712.31s of remaining time.\n",
      "\tFitting RandomForestMSE_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "\t42.51s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "\t-74605.4101\t = Validation score   (-root_mean_squared_error)\n",
      "\t3712.68s\t = Training   runtime\n",
      "\t20.64s\t = Validation runtime\n",
      "\t137.6\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 5977.85s of the 5977.29s of remaining time.\n",
      "\tFitting CatBoost_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.83%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/model.pkl\n",
      "\t-72498.4869\t = Validation score   (-root_mean_squared_error)\n",
      "\t58.33s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "\t139.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 5916.3s of the 5915.72s of remaining time.\n",
      "\tFitting ExtraTreesMSE_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "\t34.36s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "\t-73854.5708\t = Validation score   (-root_mean_squared_error)\n",
      "\t537.95s\t = Training   runtime\n",
      "\t18.74s\t = Validation runtime\n",
      "\t137.8\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 5358.03s of the 5357.46s of remaining time.\n",
      "\tFitting NeuralNetFastAI_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.87%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "\t-73705.7962\t = Validation score   (-root_mean_squared_error)\n",
      "\t869.64s\t = Training   runtime\n",
      "\t4.19s\t = Validation runtime\n",
      "\t136.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4484.74s of the 4484.15s of remaining time.\n",
      "\tFitting XGBoost_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.34%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/model.pkl\n",
      "\t-72983.096\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.07s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "\t137.9\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 4445.1s of the 4444.53s of remaining time.\n",
      "\tFitting NeuralNetTorch_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.57%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "\t-72940.5993\t = Validation score   (-root_mean_squared_error)\n",
      "\t403.9s\t = Training   runtime\n",
      "\t4.42s\t = Validation runtime\n",
      "\t136.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 4037.71s of the 4037.14s of remaining time.\n",
      "\tFitting LightGBMLarge_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.02%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "\t-73320.9127\t = Validation score   (-root_mean_squared_error)\n",
      "\t93.96s\t = Training   runtime\n",
      "\t1.39s\t = Validation runtime\n",
      "\t138.6\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 3940.47s of the 3939.9s of remaining time.\n",
      "\tFitting CatBoost_r177_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.82%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/model.pkl\n",
      "\t-72530.8287\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.36s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "\t139.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 3885.82s of the 3885.24s of remaining time.\n",
      "\tFitting NeuralNetTorch_r79_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.57%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/model.pkl\n",
      "\t-73037.2258\t = Validation score   (-root_mean_squared_error)\n",
      "\t547.17s\t = Training   runtime\n",
      "\t4.53s\t = Validation runtime\n",
      "\t136.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 3335.02s of the 3334.44s of remaining time.\n",
      "\tFitting LightGBM_r131_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.79%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/model.pkl\n",
      "\t-72991.7171\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.41s\t = Training   runtime\n",
      "\t2.55s\t = Validation runtime\n",
      "\t137.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 3194.1s of the 3193.53s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r191_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.86%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/model.pkl\n",
      "\t-73718.1621\t = Validation score   (-root_mean_squared_error)\n",
      "\t1056.1s\t = Training   runtime\n",
      "\t4.32s\t = Validation runtime\n",
      "\t136.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 2133.61s of the 2133.02s of remaining time.\n",
      "\tFitting CatBoost_r9_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.40%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/model.pkl\n",
      "\t-72598.3091\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.7s\t = Training   runtime\n",
      "\t1.49s\t = Validation runtime\n",
      "\t138.5\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 2075.01s of the 2074.44s of remaining time.\n",
      "\tFitting LightGBM_r96_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.68%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/model.pkl\n",
      "\t-72600.0752\t = Validation score   (-root_mean_squared_error)\n",
      "\t132.07s\t = Training   runtime\n",
      "\t2.98s\t = Validation runtime\n",
      "\t137.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 1938.84s of the 1938.26s of remaining time.\n",
      "\tFitting NeuralNetTorch_r22_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.57%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/model.pkl\n",
      "\t-90209.488\t = Validation score   (-root_mean_squared_error)\n",
      "\t367.84s\t = Training   runtime\n",
      "\t5.01s\t = Validation runtime\n",
      "\t135.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 1567.17s of the 1566.59s of remaining time.\n",
      "\tFitting XGBoost_r33_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=3.69%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/model.pkl\n",
      "\t-73068.6381\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.54s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "\t137.3\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 1507.65s of the 1507.08s of remaining time.\n",
      "\tFitting ExtraTrees_r42_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/utils/model_template.pkl\n",
      "\t43.62s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/model.pkl\n",
      "\t-73789.2708\t = Validation score   (-root_mean_squared_error)\n",
      "\t451.81s\t = Training   runtime\n",
      "\t21.06s\t = Validation runtime\n",
      "\t137.6\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 1032.81s of the 1032.24s of remaining time.\n",
      "\tFitting CatBoost_r137_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.69%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/model.pkl\n",
      "\t-72517.4321\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.89s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "\t139.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 977.28s of the 976.7s of remaining time.\n",
      "\tFitting NeuralNetFastAI_r102_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.88%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/model.pkl\n",
      "\t-73032.0591\t = Validation score   (-root_mean_squared_error)\n",
      "\t177.14s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "\t138.7\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 796.53s of the 795.96s of remaining time.\n",
      "\tFitting CatBoost_r13_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/utils/model_template.pkl\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.40%)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/model.pkl\n",
      "\t-72592.6311\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.43s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "\t139.1\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 733.85s of the 733.28s of remaining time.\n",
      "\tFitting RandomForest_r195_BAG_L2 with 'num_gpus': 2, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/utils/model_template.pkl\n",
      "\t46.3s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/model.pkl\n",
      "\t-74450.4575\t = Validation score   (-root_mean_squared_error)\n",
      "\t3156.35s\t = Training   runtime\n",
      "\t24.59s\t = Validation runtime\n",
      "\t137.2\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r188_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r145_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r89_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r30_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r130_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r86_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r50_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r11_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r194_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r172_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r69_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r103_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r14_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r161_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r143_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r70_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r156_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r196_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r39_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r167_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r95_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r41_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r98_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r15_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r158_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r86_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r37_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r197_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r49_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r49_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r143_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r127_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r134_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r34_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r94_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r143_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r128_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r111_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r31_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r4_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r65_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r88_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r30_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r49_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r5_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r87_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r71_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r143_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r178_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r166_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r31_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r185_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r160_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r60_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r15_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r135_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r22_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r69_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r6_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r138_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r121_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r172_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r180_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r76_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r197_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r121_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r127_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping RandomForest_r16_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r194_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r12_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r135_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r4_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping ExtraTrees_r126_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r36_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r100_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r163_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping CatBoost_r198_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_r187_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r19_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r95_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping XGBoost_r34_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping LightGBM_r42_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r1_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_r89_BAG_L2 due to lack of time remaining.\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/utils/oof.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 986.43s of the -2454.04s of remaining time.\n",
      "\tFitting WeightedEnsemble_L3 with 'num_gpus': 0, 'num_cpus': 4\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "Ensemble size: 24\n",
      "Ensemble weights: \n",
      "[0.         0.         0.         0.08333333 0.08333333 0.04166667\n",
      " 0.         0.         0.         0.125      0.         0.\n",
      " 0.04166667 0.         0.41666667 0.         0.04166667 0.04166667\n",
      " 0.         0.08333333 0.         0.         0.04166667 0.\n",
      " 0.        ]\n",
      "\t1.76s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/model.pkl\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.417, 'RandomForest_r34_BAG_L1': 0.125, 'ExtraTrees_r172_BAG_L1': 0.083, 'NeuralNetFastAI_r143_BAG_L1': 0.083, 'CatBoost_r9_BAG_L2': 0.083, 'NeuralNetFastAI_r95_BAG_L1': 0.042, 'NeuralNetFastAI_r111_BAG_L1': 0.042, 'NeuralNetTorch_BAG_L2': 0.042, 'CatBoost_r177_BAG_L2': 0.042, 'CatBoost_r137_BAG_L2': 0.042}\n",
      "\t-72460.4237\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t133.4\t = Inference  throughput (rows/s | 23567 batch size)\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 32063.95s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 133.4 rows/s (23567 batch size)\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/learner.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/predictor.pkl\n",
      "Saving AutogluonModels/ag-20240910_171549/version.txt with contents \"1.1.1\"\n",
      "Saving AutogluonModels/ag-20240910_171549/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_171549\")\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                          model     score_val              eval_metric  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L3 -72460.423724  root_mean_squared_error     279.053290  20059.416847                0.003043           0.654773            3       True         85\n",
      "1           WeightedEnsemble_L2 -72490.986540  root_mean_squared_error      95.635211   5099.747616                0.002633           0.792438            2       True         62\n",
      "2               CatBoost_BAG_L2 -72498.486929  root_mean_squared_error     271.744857  19496.905839                0.657709          58.334367            2       True         66\n",
      "3          CatBoost_r137_BAG_L2 -72517.432074  root_mean_squared_error     271.864494  19490.463722                0.777346          51.892251            2       True         81\n",
      "4          CatBoost_r177_BAG_L2 -72530.828743  root_mean_squared_error     271.704902  19489.933062                0.617754          51.361590            2       True         72\n",
      "5           CatBoost_r13_BAG_L2 -72592.631129  root_mean_squared_error     271.816045  19497.997540                0.728898          59.426068            2       True         83\n",
      "6            CatBoost_r9_BAG_L2 -72598.309055  root_mean_squared_error     272.579053  19493.271738                1.491905          54.700267            2       True         76\n",
      "7           LightGBM_r96_BAG_L2 -72600.075187  root_mean_squared_error     274.066622  19570.639457                2.979474         132.067985            2       True         77\n",
      "8             LightGBMXT_BAG_L2 -72663.851336  root_mean_squared_error     272.085316  19514.467642                0.998168          75.896170            2       True         63\n",
      "9       RandomForest_r34_BAG_L1 -72725.439869  root_mean_squared_error      10.042726    183.230050               10.042726         183.230050            1       True         56\n",
      "10          CatBoost_r13_BAG_L1 -72780.586190  root_mean_squared_error       2.966263    167.450569                2.966263         167.450569            1       True         21\n",
      "11          CatBoost_r86_BAG_L1 -72810.380945  root_mean_squared_error       1.571607     88.466259                1.571607          88.466259            1       True         48\n",
      "12         CatBoost_r167_BAG_L1 -72823.818002  root_mean_squared_error       0.992350     52.149185                0.992350          52.149185            1       True         42\n",
      "13           CatBoost_r9_BAG_L1 -72827.156811  root_mean_squared_error       1.127312     39.245821                1.127312          39.245821            1       True         14\n",
      "14         CatBoost_r177_BAG_L1 -72830.990350  root_mean_squared_error       1.173846     57.655744                1.173846          57.655744            1       True         10\n",
      "15          CatBoost_r50_BAG_L1 -72851.822037  root_mean_squared_error       1.122470     31.711310                1.122470          31.711310            1       True         29\n",
      "16          CatBoost_r70_BAG_L1 -72854.299053  root_mean_squared_error       0.905795     33.598006                0.905795          33.598006            1       True         38\n",
      "17              CatBoost_BAG_L1 -72856.080628  root_mean_squared_error       1.262802     73.372216                1.262802          73.372216            1       True          4\n",
      "18          LightGBM_r96_BAG_L1 -72879.275666  root_mean_squared_error       7.066809    102.432919                7.066809         102.432919            1       True         15\n",
      "19          CatBoost_r69_BAG_L1 -72881.444773  root_mean_squared_error       1.199993     64.956726                1.199993          64.956726            1       True         33\n",
      "20         CatBoost_r137_BAG_L1 -72882.884947  root_mean_squared_error       0.947376     59.396295                0.947376          59.396295            1       True         19\n",
      "21  NeuralNetFastAI_r143_BAG_L1 -72883.097913  root_mean_squared_error       1.316975    404.453699                1.316975         404.453699            1       True         37\n",
      "22          LightGBM_r94_BAG_L1 -72893.068546  root_mean_squared_error       2.846821     56.872800                2.846821          56.872800            1       True         57\n",
      "23          CatBoost_r49_BAG_L1 -72897.310498  root_mean_squared_error       0.484172     34.658355                0.484172          34.658355            1       True         51\n",
      "24            LightGBMXT_BAG_L1 -72930.093592  root_mean_squared_error       1.536831     44.435316                1.536831          44.435316            1       True          1\n",
      "25         LightGBM_r196_BAG_L1 -72932.637619  root_mean_squared_error      39.253707    259.457030               39.253707         259.457030            1       True         40\n",
      "26        NeuralNetTorch_BAG_L2 -72940.599332  root_mean_squared_error     275.505532  19842.473599                4.418385         403.902127            2       True         70\n",
      "27         CatBoost_r128_BAG_L1 -72955.254403  root_mean_squared_error       0.915035     30.522315                0.915035          30.522315            1       True         59\n",
      "28         LightGBM_r188_BAG_L1 -72977.875149  root_mean_squared_error       2.558837     53.495589                2.558837          53.495589            1       True         23\n",
      "29               XGBoost_BAG_L2 -72983.095974  root_mean_squared_error     273.350142  19474.641195                2.262995          36.069723            2       True         69\n",
      "30  NeuralNetFastAI_r111_BAG_L1 -72987.926603  root_mean_squared_error       0.927769    137.420976                0.927769         137.420976            1       True         60\n",
      "31         LightGBM_r131_BAG_L2 -72991.717113  root_mean_squared_error     273.633285  19575.976555                2.546137         137.405083            2       True         74\n",
      "32         LightGBM_r130_BAG_L1 -73023.713131  root_mean_squared_error       1.662494     49.235780                1.662494          49.235780            1       True         27\n",
      "33  NeuralNetFastAI_r102_BAG_L2 -73032.059072  root_mean_squared_error     272.371606  19615.709168                1.284458         177.137697            2       True         82\n",
      "34    NeuralNetTorch_r79_BAG_L2 -73037.225818  root_mean_squared_error     275.612207  19985.744789                4.525060         547.173318            2       True         73\n",
      "35           XGBoost_r33_BAG_L2 -73068.638113  root_mean_squared_error     274.061034  19494.115884                2.973887          55.544413            2       True         79\n",
      "36              LightGBM_BAG_L2 -73072.628857  root_mean_squared_error     271.834829  19507.578142                0.747681          69.006670            2       True         64\n",
      "37         LightGBM_r131_BAG_L1 -73091.837558  root_mean_squared_error       4.008784     86.873657                4.008784          86.873657            1       True         12\n",
      "38   NeuralNetFastAI_r37_BAG_L1 -73093.339304  root_mean_squared_error       2.373508    721.492208                2.373508         721.492208            1       True         49\n",
      "39          LightGBM_r15_BAG_L1 -73117.183994  root_mean_squared_error       2.329402     60.137213                2.329402          60.137213            1       True         46\n",
      "40         LightGBM_r161_BAG_L1 -73118.840148  root_mean_squared_error       8.645492    111.585891                8.645492         111.585891            1       True         36\n",
      "41   NeuralNetFastAI_r95_BAG_L1 -73125.646430  root_mean_squared_error       7.291174   1884.517758                7.291174        1884.517758            1       True         43\n",
      "42  NeuralNetFastAI_r156_BAG_L1 -73151.185468  root_mean_squared_error       0.857077    138.620222                0.857077         138.620222            1       True         39\n",
      "43  NeuralNetFastAI_r145_BAG_L1 -73170.293019  root_mean_squared_error       7.212402   1781.514849                7.212402        1781.514849            1       True         24\n",
      "44       ExtraTrees_r172_BAG_L1 -73176.860436  root_mean_squared_error      13.186659    259.414993               13.186659         259.414993            1       True         32\n",
      "45              LightGBM_BAG_L1 -73184.429334  root_mean_squared_error       1.027969     41.975313                1.027969          41.975313            1       True          2\n",
      "46  NeuralNetFastAI_r191_BAG_L1 -73200.091200  root_mean_squared_error       3.946204    940.766338                3.946204         940.766338            1       True         13\n",
      "47           XGBoost_r89_BAG_L1 -73265.368505  root_mean_squared_error       1.279491     23.929931                1.279491          23.929931            1       True         25\n",
      "48  NeuralNetFastAI_r134_BAG_L1 -73276.007437  root_mean_squared_error       1.013496    148.595501                1.013496         148.595501            1       True         55\n",
      "49           XGBoost_r98_BAG_L1 -73296.959936  root_mean_squared_error       1.193182     80.778733                1.193182          80.778733            1       True         45\n",
      "50       NeuralNetFastAI_BAG_L1 -73311.213225  root_mean_squared_error       4.031438    901.731218                4.031438         901.731218            1       True          6\n",
      "51         LightGBM_r143_BAG_L1 -73311.949148  root_mean_squared_error       4.165450     93.067135                4.165450          93.067135            1       True         53\n",
      "52  NeuralNetFastAI_r103_BAG_L1 -73319.030338  root_mean_squared_error       3.923035    980.928424                3.923035         980.928424            1       True         34\n",
      "53         LightGBMLarge_BAG_L2 -73320.912687  root_mean_squared_error     272.473841  19532.529087                1.386693          93.957615            2       True         71\n",
      "54  NeuralNetFastAI_r102_BAG_L1 -73333.228197  root_mean_squared_error       0.960590    164.354710                0.960590         164.354710            1       True         20\n",
      "55   NeuralNetTorch_r143_BAG_L1 -73413.923533  root_mean_squared_error       1.445490    699.162435                1.445490         699.162435            1       True         58\n",
      "56    NeuralNetTorch_r41_BAG_L1 -73441.522004  root_mean_squared_error       1.549306    744.568730                1.549306         744.568730            1       True         44\n",
      "57    NeuralNetTorch_r79_BAG_L1 -73470.676154  root_mean_squared_error       1.534464    654.094755                1.534464         654.094755            1       True         11\n",
      "58        NeuralNetTorch_BAG_L1 -73476.881657  root_mean_squared_error       1.396882    765.555856                1.396882         765.555856            1       True          8\n",
      "59    NeuralNetTorch_r14_BAG_L1 -73511.607865  root_mean_squared_error       1.350593    473.556476                1.350593         473.556476            1       True         35\n",
      "60     RandomForest_r127_BAG_L1 -73531.771443  root_mean_squared_error      13.899932    359.536268               13.899932         359.536268            1       True         54\n",
      "61         LightGBMLarge_BAG_L1 -73561.223206  root_mean_squared_error       1.443347     50.301448                1.443347          50.301448            1       True          9\n",
      "62           XGBoost_r33_BAG_L1 -73599.187931  root_mean_squared_error       1.374338     34.520884                1.374338          34.520884            1       True         17\n",
      "63               XGBoost_BAG_L1 -73688.990314  root_mean_squared_error       1.334688     23.078558                1.334688          23.078558            1       True          7\n",
      "64       NeuralNetFastAI_BAG_L2 -73705.796160  root_mean_squared_error     275.279564  20308.212898                4.192417         869.641426            2       True         68\n",
      "65  NeuralNetFastAI_r191_BAG_L2 -73718.162082  root_mean_squared_error     275.410175  20494.674089                4.323027        1056.102618            2       True         75\n",
      "66   NeuralNetFastAI_r11_BAG_L1 -73733.714631  root_mean_squared_error       7.081065   1740.046201                7.081065        1740.046201            1       True         30\n",
      "67          XGBoost_r194_BAG_L1 -73750.208118  root_mean_squared_error       0.996987     26.078562                0.996987          26.078562            1       True         31\n",
      "68        ExtraTrees_r42_BAG_L2 -73789.270840  root_mean_squared_error     292.147102  19890.382346               21.059955         451.810874            2       True         80\n",
      "69         ExtraTreesMSE_BAG_L2 -73854.570848  root_mean_squared_error     289.823727  19976.519401               18.736580         537.947929            2       True         67\n",
      "70      RandomForest_r39_BAG_L1 -74335.371596  root_mean_squared_error      14.957736    319.026653               14.957736         319.026653            1       True         41\n",
      "71        ExtraTrees_r49_BAG_L1 -74363.793761  root_mean_squared_error      12.419104     39.131806               12.419104          39.131806            1       True         52\n",
      "72     RandomForest_r195_BAG_L2 -74450.457549  root_mean_squared_error     295.673352  22594.922040               24.586205        3156.350569            2       True         84\n",
      "73       RandomForestMSE_BAG_L2 -74605.410055  root_mean_squared_error     291.727564  23151.251988               20.640416        3712.680517            2       True         65\n",
      "74        ExtraTrees_r42_BAG_L1 -75851.319186  root_mean_squared_error      12.067334    177.288454               12.067334         177.288454            1       True         18\n",
      "75         ExtraTreesMSE_BAG_L1 -76043.533285  root_mean_squared_error      12.395598    249.440145               12.395598         249.440145            1       True          5\n",
      "76     RandomForest_r195_BAG_L1 -76049.939384  root_mean_squared_error      15.657525    341.464171               15.657525         341.464171            1       True         22\n",
      "77       RandomForestMSE_BAG_L1 -76887.772050  root_mean_squared_error      12.450263    360.307063               12.450263         360.307063            1       True          3\n",
      "78    NeuralNetTorch_r30_BAG_L1 -82470.676308  root_mean_squared_error       1.633261    559.553121                1.633261         559.553121            1       True         26\n",
      "79   NeuralNetTorch_r197_BAG_L1 -86365.225292  root_mean_squared_error       1.242618    245.955312                1.242618         245.955312            1       True         50\n",
      "80   NeuralNetTorch_r158_BAG_L1 -90209.459491  root_mean_squared_error       1.535660    444.695758                1.535660         444.695758            1       True         47\n",
      "81    NeuralNetTorch_r22_BAG_L2 -90209.487958  root_mean_squared_error     276.094049  19806.412071                5.006902         367.840599            2       True         78\n",
      "82    NeuralNetTorch_r22_BAG_L1 -90209.535136  root_mean_squared_error       1.299191    307.069428                1.299191         307.069428            1       True         16\n",
      "83    NeuralNetTorch_r86_BAG_L1 -90209.558475  root_mean_squared_error       1.469808    326.626285                1.469808         326.626285            1       True         28\n",
      "84    NeuralNetTorch_r31_BAG_L1 -90229.785190  root_mean_squared_error       1.224614     53.012049                1.224614          53.012049            1       True         61\n",
      "Number of models trained: 85\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_XGBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "('category', ['text_as_category'])  :  1 | ['engine']\n",
      "('int', [])                         :  3 | ['id', 'model_year', 'milage']\n",
      "('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "('int', ['bool'])                   :  1 | ['clean_title']\n",
      "('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20240910_171549SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label,eval_metric ='root_mean_squared_error',\n",
    "                             problem_type=\"regression\").fit(train,presets='best_quality',\n",
    "                                                           time_limit=3600*11,verbosity=3,\n",
    "                                                           excluded_model_types=['KNN'],\n",
    "                                                           ag_args_fit={'num_gpus': 1}\n",
    "                                                      )\n",
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2197f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T04:56:52.470484Z",
     "iopub.status.busy": "2024-09-11T04:56:52.469073Z",
     "iopub.status.idle": "2024-09-11T04:56:52.515884Z",
     "shell.execute_reply": "2024-09-11T04:56:52.514991Z"
    },
    "papermill": {
     "duration": 0.263802,
     "end_time": "2024-09-11T04:56:52.517953",
     "exception": false,
     "start_time": "2024-09-11T04:56:52.254151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-72460.423724</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>279.053290</td>\n",
       "      <td>20059.416847</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.654773</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-72490.986540</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>95.635211</td>\n",
       "      <td>5099.747616</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.792438</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost_BAG_L2</td>\n",
       "      <td>-72498.486929</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>271.744857</td>\n",
       "      <td>19496.905839</td>\n",
       "      <td>0.657709</td>\n",
       "      <td>58.334367</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost_r137_BAG_L2</td>\n",
       "      <td>-72517.432074</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>271.864494</td>\n",
       "      <td>19490.463722</td>\n",
       "      <td>0.777346</td>\n",
       "      <td>51.892251</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost_r177_BAG_L2</td>\n",
       "      <td>-72530.828743</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>271.704902</td>\n",
       "      <td>19489.933062</td>\n",
       "      <td>0.617754</td>\n",
       "      <td>51.361590</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>NeuralNetTorch_r158_BAG_L1</td>\n",
       "      <td>-90209.459491</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.535660</td>\n",
       "      <td>444.695758</td>\n",
       "      <td>1.535660</td>\n",
       "      <td>444.695758</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NeuralNetTorch_r22_BAG_L2</td>\n",
       "      <td>-90209.487958</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>276.094049</td>\n",
       "      <td>19806.412071</td>\n",
       "      <td>5.006902</td>\n",
       "      <td>367.840599</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NeuralNetTorch_r22_BAG_L1</td>\n",
       "      <td>-90209.535136</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.299191</td>\n",
       "      <td>307.069428</td>\n",
       "      <td>1.299191</td>\n",
       "      <td>307.069428</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>NeuralNetTorch_r86_BAG_L1</td>\n",
       "      <td>-90209.558475</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.469808</td>\n",
       "      <td>326.626285</td>\n",
       "      <td>1.469808</td>\n",
       "      <td>326.626285</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NeuralNetTorch_r31_BAG_L1</td>\n",
       "      <td>-90229.785190</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.224614</td>\n",
       "      <td>53.012049</td>\n",
       "      <td>1.224614</td>\n",
       "      <td>53.012049</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model     score_val              eval_metric  \\\n",
       "0          WeightedEnsemble_L3 -72460.423724  root_mean_squared_error   \n",
       "1          WeightedEnsemble_L2 -72490.986540  root_mean_squared_error   \n",
       "2              CatBoost_BAG_L2 -72498.486929  root_mean_squared_error   \n",
       "3         CatBoost_r137_BAG_L2 -72517.432074  root_mean_squared_error   \n",
       "4         CatBoost_r177_BAG_L2 -72530.828743  root_mean_squared_error   \n",
       "..                         ...           ...                      ...   \n",
       "80  NeuralNetTorch_r158_BAG_L1 -90209.459491  root_mean_squared_error   \n",
       "81   NeuralNetTorch_r22_BAG_L2 -90209.487958  root_mean_squared_error   \n",
       "82   NeuralNetTorch_r22_BAG_L1 -90209.535136  root_mean_squared_error   \n",
       "83   NeuralNetTorch_r86_BAG_L1 -90209.558475  root_mean_squared_error   \n",
       "84   NeuralNetTorch_r31_BAG_L1 -90229.785190  root_mean_squared_error   \n",
       "\n",
       "    pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0      279.053290  20059.416847                0.003043           0.654773   \n",
       "1       95.635211   5099.747616                0.002633           0.792438   \n",
       "2      271.744857  19496.905839                0.657709          58.334367   \n",
       "3      271.864494  19490.463722                0.777346          51.892251   \n",
       "4      271.704902  19489.933062                0.617754          51.361590   \n",
       "..            ...           ...                     ...                ...   \n",
       "80       1.535660    444.695758                1.535660         444.695758   \n",
       "81     276.094049  19806.412071                5.006902         367.840599   \n",
       "82       1.299191    307.069428                1.299191         307.069428   \n",
       "83       1.469808    326.626285                1.469808         326.626285   \n",
       "84       1.224614     53.012049                1.224614          53.012049   \n",
       "\n",
       "    stack_level  can_infer  fit_order  \n",
       "0             3       True         85  \n",
       "1             2       True         62  \n",
       "2             2       True         66  \n",
       "3             2       True         81  \n",
       "4             2       True         72  \n",
       "..          ...        ...        ...  \n",
       "80            1       True         47  \n",
       "81            2       True         78  \n",
       "82            1       True         16  \n",
       "83            1       True         28  \n",
       "84            1       True         61  \n",
       "\n",
       "[85 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f4947",
   "metadata": {
    "papermill": {
     "duration": 0.22455,
     "end_time": "2024-09-11T04:56:52.964816",
     "exception": false,
     "start_time": "2024-09-11T04:56:52.740266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🏅 **Submission**\n",
    "\n",
    "Finally, we'll generate the submission file. This file will be in the required format with id and price columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d0950a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T04:56:53.448918Z",
     "iopub.status.busy": "2024-09-11T04:56:53.448029Z",
     "iopub.status.idle": "2024-09-11T05:08:30.691077Z",
     "shell.execute_reply": "2024-09-11T05:08:30.690071Z"
    },
    "papermill": {
     "duration": 697.511248,
     "end_time": "2024-09-11T05:08:30.693266",
     "exception": false,
     "start_time": "2024-09-11T04:56:53.182018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r128_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r13_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r167_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r49_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r50_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r69_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r70_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r86_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r172_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r42_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/ExtraTrees_r49_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r130_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r131_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r15_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r161_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r188_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r196_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r94_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/LightGBM_r96_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r102_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r103_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r111_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r11_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r134_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r145_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r156_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r191_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r37_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetFastAI_r95_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r143_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r14_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r158_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r197_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r22_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r30_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r31_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r41_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r79_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_r86_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r127_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r195_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r34_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/RandomForest_r39_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r194_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r33_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r89_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/XGBoost_r98_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r137_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r177_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/CatBoost_r9_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240910_171549/models/WeightedEnsemble_L3/model.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    18676.458984\n",
       "1    85042.539062\n",
       "2    58277.531250\n",
       "3    30024.185547\n",
       "4    29510.189453\n",
       "Name: price, dtype: float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predictor.predict(test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380d36ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T05:08:31.186936Z",
     "iopub.status.busy": "2024-09-11T05:08:31.185201Z",
     "iopub.status.idle": "2024-09-11T05:08:31.207920Z",
     "shell.execute_reply": "2024-09-11T05:08:31.206994Z"
    },
    "papermill": {
     "duration": 0.283558,
     "end_time": "2024-09-11T05:08:31.209977",
     "exception": false,
     "start_time": "2024-09-11T05:08:30.926419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18676.458984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85042.539062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58277.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30024.185547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29510.189453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          price\n",
       "0  18676.458984\n",
       "1  85042.539062\n",
       "2  58277.531250\n",
       "3  30024.185547\n",
       "4  29510.189453"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(y_pred,columns=['price'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081e065e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T05:08:31.673443Z",
     "iopub.status.busy": "2024-09-11T05:08:31.672555Z",
     "iopub.status.idle": "2024-09-11T05:08:31.754720Z",
     "shell.execute_reply": "2024-09-11T05:08:31.753789Z"
    },
    "papermill": {
     "duration": 0.313609,
     "end_time": "2024-09-11T05:08:31.756772",
     "exception": false,
     "start_time": "2024-09-11T05:08:31.443163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188533</td>\n",
       "      <td>43878.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188534</td>\n",
       "      <td>43878.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188535</td>\n",
       "      <td>43878.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188536</td>\n",
       "      <td>43878.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188537</td>\n",
       "      <td>43878.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      price\n",
       "0  188533  43878.016\n",
       "1  188534  43878.016\n",
       "2  188535  43878.016\n",
       "3  188536  43878.016\n",
       "4  188537  43878.016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol=pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\n",
    "sol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b642c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T05:08:32.211772Z",
     "iopub.status.busy": "2024-09-11T05:08:32.211359Z",
     "iopub.status.idle": "2024-09-11T05:08:32.222334Z",
     "shell.execute_reply": "2024-09-11T05:08:32.221397Z"
    },
    "papermill": {
     "duration": 0.241515,
     "end_time": "2024-09-11T05:08:32.224467",
     "exception": false,
     "start_time": "2024-09-11T05:08:31.982952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188533</td>\n",
       "      <td>18676.458984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188534</td>\n",
       "      <td>85042.539062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188535</td>\n",
       "      <td>58277.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188536</td>\n",
       "      <td>30024.185547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188537</td>\n",
       "      <td>29510.189453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         price\n",
       "0  188533  18676.458984\n",
       "1  188534  85042.539062\n",
       "2  188535  58277.531250\n",
       "3  188536  30024.185547\n",
       "4  188537  29510.189453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol['price']=df['price']\n",
    "sol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d08022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T05:08:32.694200Z",
     "iopub.status.busy": "2024-09-11T05:08:32.693535Z",
     "iopub.status.idle": "2024-09-11T05:08:32.954315Z",
     "shell.execute_reply": "2024-09-11T05:08:32.953469Z"
    },
    "papermill": {
     "duration": 0.49346,
     "end_time": "2024-09-11T05:08:32.956671",
     "exception": false,
     "start_time": "2024-09-11T05:08:32.463211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sol.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759a542",
   "metadata": {
    "papermill": {
     "duration": 0.225683,
     "end_time": "2024-09-11T05:08:33.408154",
     "exception": false,
     "start_time": "2024-09-11T05:08:33.182471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🎯 **Conclusion**\n",
    "\n",
    "In this kernel, we successfully built a used car price prediction model using AutoGluon, efficiently utilizing GPU resources. Keep experimenting and refining your model to climb the leaderboard! 🏆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3984b1b",
   "metadata": {
    "papermill": {
     "duration": 0.222783,
     "end_time": "2024-09-11T05:08:33.904929",
     "exception": false,
     "start_time": "2024-09-11T05:08:33.682146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9057646,
     "sourceId": 76728,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42839.855402,
   "end_time": "2024-09-11T05:08:39.369735",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-10T17:14:39.514333",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
